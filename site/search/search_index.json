{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"TopOMetry is a high-level python library to explore data topology. It allows learning topological metrics, dimensionality reduced basis and graphs from data, as well to visualize them with different layout optimization algorithms. The main objective is to achieve approximations of the Laplace-Beltrami Operator , a natural way to describe data geometry and its high-dimensional topology. TopOMetry is designed to handle large-scale data matrices containing extreme topological diversity, such as those generated from single-cell omics , and can be used to perform topology-preserving visualizations. TopoMetry main class is the TopoGraph object. In a TopoGraph , topological metrics are recovered with diffusion harmonics or Continuous-k-Nearest-Neighbors, and used to obtain topological basis (multiscale Diffusion Maps and/or diffuse or continuous versions of Laplacian Eigenmaps). On top of these basis, new graphs can be learned using k-nearest-neighbors graphs or additional topological operators. The learned metrics, basis and graphs are stored as different attributes of the TopoGraph object. Finally, different visualizations of the learned topology can be optimized with pyMDE by solving a Minimum-Distortion Embedding problem. TopOMetry also implements an adapted, non-uniform version of the seminal Uniform Manifold Approximation and Projection (UMAP) for graph layout optimization (we call it MAP for short). Alternatively, you can use TopOMetry to add topological information to your favorite workflow by using its dimensionality reduced basis to compute k-nearest-neighbors instead of PCA.","title":"Welcome!"},{"location":"TopOMetry_Intro_pbmc_10k/","text":"Analyzing PBMCs single-cell data with TopOMetry TopOMetry is intended for the topological analysis and visualization of high-dimensional data. In this notebook, I showcase how it can be used for the analysis of single-cell data, a challenging data modality which has been pushing representation learning to new frontiers. We'll be using a dataset containing around 6,000 individually RNA sequenced cells, consisting of peripherical blood mononuclear cells extracted from a healthy donor. To assist us throghout this analysis, we'll be using scanpy , a scalable toolkit for analyzing single-cell RNA sequencing data. In particular, we'll use the anndata framework for storing single-cell data and its plotting API for some plotting. Install and load libraries # Install pre-requisites and scanpy (pre-requisites for scanpy are python-igraph and leidenalg) #!pip install nmslib annoy scipy scanpy numba kneed pymde python-igraph leidenalg scanpy # Install pre-release of TopOMetry #!pip install -i https://test.pypi.org/simple/ topo import numpy as np import pandas as pd import scanpy as sc import pymde import topo as tp /usr/local/lib/python3.8/dist-packages/dask/config.py:161: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details. data = yaml.load(f.read()) or {} These are some scanpy helper functions: sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3) sc.logging.print_header() sc.settings.set_figure_params(dpi=60, dpi_save=300, facecolor='white') scanpy==1.6.0 anndata==0.7.6 umap==0.4.6 numpy==1.18.5 scipy==1.5.3 pandas==1.1.3 scikit-learn==0.23.1 statsmodels==0.11.1 python-igraph==0.9.1 louvain==0.7.0 leidenalg==0.8.1 Load data and QC Download data from 10X genomics website : !mkdir data !wget https://cf.10xgenomics.com/samples/cell-exp/6.0.0/SC3_v3_NextGem_DI_CellPlex_Human_PBMC_10K_PBMCs_human_1/SC3_v3_NextGem_DI_CellPlex_Human_PBMC_10K_PBMCs_human_1_count_sample_feature_bc_matrix.h5 -O data/SC3_v3_NextGem_DI_CellPlex_Human_PBMC_10K_PBMCs_human_1_count_sample_feature_bc_matrix.h5 !mkdir write mkdir: cannot create directory \u2018data\u2019: File exists --2021-05-11 21:43:42-- https://cf.10xgenomics.com/samples/cell-exp/6.0.0/SC3_v3_NextGem_DI_CellPlex_Human_PBMC_10K_PBMCs_human_1/SC3_v3_NextGem_DI_CellPlex_Human_PBMC_10K_PBMCs_human_1_count_sample_feature_bc_matrix.h5 Resolving cf.10xgenomics.com (cf.10xgenomics.com)... 2606:4700::6812:1ad, 2606:4700::6812:ad, 104.18.0.173, ... Connecting to cf.10xgenomics.com (cf.10xgenomics.com)|2606:4700::6812:1ad|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 15239045 (15M) [binary/octet-stream] Saving to: \u2018data/SC3_v3_NextGem_DI_CellPlex_Human_PBMC_10K_PBMCs_human_1_count_sample_feature_bc_matrix.h5\u2019 data/SC3_v3_NextGem 100%[===================>] 14.53M 6.05MB/s in 2.4s 2021-05-11 21:43:45 (6.05 MB/s) - \u2018data/SC3_v3_NextGem_DI_CellPlex_Human_PBMC_10K_PBMCs_human_1_count_sample_feature_bc_matrix.h5\u2019 saved [15239045/15239045] mkdir: cannot create directory \u2018write\u2019: File exists wd = '/home/davi/Documents/Bioinfo/pheno_tutorials/pbmc10k/' results_file = 'write/pbmc10k.h5ad' # the file that will store the analysis results Read in the data with Scanpy: adata = sc.read_10x_h5( 'data/SC3_v3_NextGem_DI_CellPlex_Human_PBMC_10K_PBMCs_human_1_count_sample_feature_bc_matrix.h5') adata.var_names_make_unique() adata reading data/SC3_v3_NextGem_DI_CellPlex_Human_PBMC_10K_PBMCs_human_1_count_sample_feature_bc_matrix.h5 Variable names are not unique. To make them unique, call `.var_names_make_unique`. (0:00:00) Variable names are not unique. To make them unique, call `.var_names_make_unique`. AnnData object with n_obs \u00d7 n_vars = 6437 \u00d7 36601 var: 'gene_ids', 'feature_types', 'genome' This data contains 6,437 cells with 36,601 sequenced genes each. A great deal of this is just experimental noise, so we need to perform some quality control. Here, we'll do the default quality-control analysis: # Default QC sc.pp.filter_cells(adata, min_genes=200) sc.pp.filter_genes(adata, min_cells=3) adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes as 'mt' sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True) sc.pl.violin(adata, ['n_genes', 'total_counts', 'pct_counts_mt'], jitter=0.4, multi_panel=True) filtered out 13 cells that have less than 200 genes expressed filtered out 17855 genes that are detected in less than 3 cells ... storing 'feature_types' as categorical ... storing 'genome' as categorical Actually perform some filtering: adata = adata[adata.obs.n_genes < 3000, :] adata = adata[adata.obs.total_counts < 10000, :] adata = adata[adata.obs.pct_counts_mt < 20, :] adata View of AnnData object with n_obs \u00d7 n_vars = 5942 \u00d7 18746 obs: 'n_genes', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt' var: 'gene_ids', 'feature_types', 'genome', 'n_cells', 'mt', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts' Default Scanpy workflow (PCA-based) Here, for brevity sake, we'll perform the default scanpy workflow. For more information about the analysis of single-cell data and proposed workflows, we direct the interested user to Scanpy tutorials and this excellent best-practices review . In short, this involves normalizing the library size of each sequenced cell by the total number of detected mRNA molecules, then logarithmizing it. Next, genes with high expression mean and high dispersion are selected as highly-variable genes, and stored for downstream analysis. Data is then scaled and mean-centered. The default workflow further involves computing PCA, and then using top principal components to compute a neighborhood graph. From this graph, it is possible to cluster cells with the leiden algorithm, and to obtain lower-dimensional embeddings with UMAP. sc.pp.normalize_total(adata, target_sum=1e4) sc.pp.log1p(adata) sc.pp.highly_variable_genes(adata, min_mean=0.125, max_mean=8, min_disp=0.3) adata.raw = adata adata = adata[:, adata.var.highly_variable] sc.pp.scale(adata, max_value=10) sc.tl.pca(adata) sc.pp.neighbors(adata) sc.tl.leiden(adata) sc.tl.umap(adata) sc.pl.embedding(adata, basis='umap', color=['leiden'], size=5) normalizing counts per cell finished (0:00:00) extracting highly variable genes /home/davi/.local/lib/python3.8/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata) finished (0:00:00) --> added 'highly_variable', boolean vector (adata.var) 'means', float vector (adata.var) 'dispersions', float vector (adata.var) 'dispersions_norm', float vector (adata.var) ... as `zero_center=True`, sparse input is densified and may lead to large memory consumption computing PCA on highly variable genes with n_comps=50 /home/davi/.local/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py:806: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata) finished (0:00:01) computing neighbors using 'X_pca' with n_pcs = 50 finished: added to `.uns['neighbors']` `.obsp['distances']`, distances for each pair of neighbors `.obsp['connectivities']`, weighted adjacency matrix (0:00:02) running Leiden clustering finished: found 13 clusters and added 'leiden', the cluster labels (adata.obs, categorical) (0:00:00) computing UMAP finished: added 'X_umap', UMAP coordinates (adata.obsm) (0:00:07) This looks neat. Let's see how it compares to the TopOMetry workflow. Topological analysis with TopOMetry Now that we have concluded the default Scanpy analysis, let's perform some topological analyses with TopOMetry. For this, we will start up a TopOGraph object with some data, and populate its slots as we proceed with the analysis. These 4 steps will be followed: Learn topological metrics - this can be done with diffusion harmonics or continuous k-nearest-neighbors (ckNN) Build a topological basis - this can be done with multiscale diffusion maps or a continuous Laplacian Eigenmap Extract a topological graph - again, this can be done with diffusion harmonics or ckNN Optimize a topological layout - this can be done with a Minimum Distortion Embedding or a Manifold Approximation and Projection For now, let's start with the default TopOMetry analysis - using diffusion harmonics, multiscale maps and graphs. Fit a topological basis # Convert the data - we'll use a csr_matrix, for speed from scipy.sparse import csr_matrix data = csr_matrix(adata.X) # scanpy stores the gene expression data at adata.X # Start up the TopOGraph tg = tp.models.TopOGraph(n_jobs=12, # set this to the highest number of available threads plot_spectrum=True) tg.fit(data) Building topological basis... Topological basis fitted with diffusion mappings in 39.156711 (sec) TopoGraph object with 5942 samples and 1338 observations and: Diffusion basis fitted - .DiffBasis Multiscale Diffusion Maps fitted - .MSDiffMap Active basis: diffusion basis. Active graph: diff graph. TopOMetry allows the user to visualize the dataset diffusion eigenspectrum when building multiscale diffusion maps. It automatically estimates the dataset dimensionality and selects and adequate number of diffusion components to scale and store for downstream analysis at TopoGraph.MSDiffMap . Users are also allowed to rescale the diffusion components to another number of components, if they want to obtain less or further detail on their dataset. As rule of thumb, more components lead to more detailed analyses, but can result in very disconnected embeddings. After re-scaling our multiscale diffusion maps, we'll add it to our AnnData object. # Convenience function to re-scale to user's chosen number of components tg.MSDiffMap = tg.DiffBasis.rescale(n_eigs=50) # Add the multiscale diffusion maps to adata as a lower-dimensional representation adata.obsm['X_ms_diff_map'] = tg.MSDiffMap Fit the topological graph Let's take a look at our TopOGraph object: tg TopoGraph object with 5942 samples and 1338 observations and: Diffusion basis fitted - .DiffBasis Multiscale Diffusion Maps fitted - .MSDiffMap Active basis: diffusion basis. Active graph: diff graph. After learning a topological basis, extracting an associated topological graph from the active basis is as simple as running TopOGraph.transform() . Note that the graph will be computed as per the TopOGraph.graph parameter (default diff , diffusion harmonics), on the specified TopOGraph.basis parameter (the active basis). db_diff_graph = tg.transform() Building topological graph... Topological graph extracted in = 0.235041 (sec) Optimize the topological graph layout Now that we have learned a topological graph from our topological basis, we need to optimize its layout in order to visualize it. TopOMetry offers two options for that task: MAP - Manifold Approximation and Projection, a generalized version of UMAP that does not hold the uniform distribution assumption. It's goal is to minimize the divergence between a given basis and a topological graph. MDE - Minimum Distortion Embedding, a recently proposed dimensionality reduction method that generalized the problem of effective data visualization through convex optimization. Naturally, you may perform visualization with any other method, such as t-SNE, PHATE or autoencoders, and still take advantage of TopOMetry by simply using the topological basis as input instead of raw data. Alternatively, if your chosen method takes as input a matrix of similarities, you can use the topological graph. First, we'll perform layout optimization with the MAP approach: # Minimize divergence between the diffusion basis and diffusion graph emb_dbmap, aux = tg.MAP(tg.MSDiffMap, db_diff_graph) # Add learned embedding to adata adata.obsm['X_dbMAP'] = emb_dbmap # Plot with Scanpy sc.pl.embedding(adata, basis='dbMAP', color=['leiden'], edges=False, size=5) Fuzzy layout optimization embedding in = 6.311060 (sec) Next, we'll perform the layout optimization with pyMDE. This involves building a pyMDE graph from our topological graph (so to be compatible with pytorch), setting up an MDE problem, and effectively optimizing the embedding. TopOMetry has pyMDE functions built-in in the topo.lt module. A separate tutorial exploring the wide array of options involved in MDE optimization is in the making. For now, let's run MDE with the default parameters: # Layout optimization with Minimum Distortion Embedding # Set up MDE problem db_diff_mde = tg.MDE(db_diff_graph, constraint=None) db_diff_emb = db_diff_mde.embed(verbose=False, snapshot_every=1) # Add to adata adata.obsm['X_db_diff_MDE'] = db_diff_emb # Plot with scanpy sc.pl.embedding(adata, basis='db_diff_MDE', color=['leiden'], edges=False, size=5) pyMDE allows us to visualize how data looks like during the optimization process: # Explore how data behaves during optimization db_diff_mde.play(savepath= wd + 'pbmc10k_db_diff_MDE.gif',marker_size=2, color_by=adata.obs['leiden'], figsize_inches=(2,2)) HBox(children=(FloatProgress(value=0.0, max=301.0), HTML(value=''))) Let's now see what we have got inside the tg object storing our TopOGraph: tg TopoGraph object with 5942 samples and 1338 observations and: Diffusion basis fitted - .DiffBasis Multiscale Diffusion Maps fitted - .MSDiffMap Diffusion graph fitted - .DiffGraph Active basis: diffusion basis. Active graph: diff graph. Exploring alternative topologies with TopOMetry Now that we have explored some of data topology with the default TopOMetry analysis, we can also explore it under continuous nearest-neighbors topologies. We will explore the following combinations: 1 - Diffusion basis, diffusion graph (default, as we did above) 2 - Diffusion basis, continuous graph 3 - Continuous basis, diffusion graph 4 - Continuous basis, continuous graph Although all these models render similar results, they give you, the end user, flexibility to find the better suited combination for your data. From our experience, the default combination tends to have more robust and significative results throghout a myriad of datasets, but you have a choice :) Let's now try combination (2). Using our active basis, let's change our active graph model to cknn. tg.graph = 'cknn' db_cknn_graph = tg.transform() Building topological graph... Topological graph extracted in = 0.915521 (sec) Again, let's perform layout optimization with Minimum Distortion Embedding # Set up MDE problem db_cknn_mde = tg.MDE(db_cknn_graph) db_cknn_emb = db_cknn_mde.embed(verbose=False, snapshot_every=1) # Add to adata adata.obsm['X_db_cknn_MDE'] = db_cknn_emb # Plot with scanpy sc.pl.embedding(adata, basis='db_cknn_MDE', color=['leiden'], edges=False, size=5) # Explore how data behaves during optimization db_cknn_mde.play(savepath= wd + 'pbmc10k_db_cknn_MDE.gif', marker_size=2, color_by=adata.obs['leiden'], axis_limits=[-5, 5], figsize_inches=(2,2)) HBox(children=(FloatProgress(value=0.0, max=301.0), HTML(value=''))) As we can see, we still get more or less the same clusters distribution. However, we also see that this model is more sensitive to outliers, represented by the sparsely distributed cells without any relation to any cluster. Now, let's explore how a continuous basis looks like. For that, we'll need to change the basis argument of our tg TopOGraph object, and fit the model once again. The previously computed diffusion basis is not lost in this process. Note that when computing a continuous basis, a continuous version of Laplacian Eigenmaps is used to embed the metric into latent dimensions. We'll start with combination (3), using a continuous basis and a diffusion graph. tg.basis = 'continuous' tg.fit(data) tg.graph = 'diff' cb_diff_graph = tg.transform() # Add the continuous Laplacian Eigenmaps to adata adata.obsm['X_c_lapmap'] = tg.CLapMap Building topological basis... /usr/local/lib/python3.8/dist-packages/sklearn/manifold/_spectral_embedding.py:236: UserWarning: Graph is not fully connected, spectral embedding may not work as expected. warnings.warn(\"Graph is not fully connected, spectral embedding\" Topological basis fitted with continuous mappings in 116.046612 (sec) Building topological graph... Topological graph extracted in = 0.289549 (sec) # Set up MDE problem cb_diff_mde = tg.MDE(cb_diff_graph) cb_diff_emb = cb_diff_mde.embed(verbose=False, snapshot_every=1) # Add to adata adata.obsm['X_cb_diff_MDE'] = cb_diff_emb # Plot with scanpy sc.pl.embedding(adata, basis='cb_diff_MDE', color=['leiden'], edges=False, size=5) # Explore how data behaves during optimization cb_diff_mde.play(savepath=wd + 'pbmc10k_cb_diff_MDE.gif', marker_size=2, color_by=adata.obs['leiden'], axis_limits=[-3, 3], figsize_inches=(2,2)) HBox(children=(FloatProgress(value=0.0, max=301.0), HTML(value=''))) Now let's go through with combination (4), using a continuous basis and a continuous graph. Because we have already fitted the continuous basis in our tg object, we only need to compute the graph. tg.graph = 'cknn' cb_cknn_graph = tg.transform() # Set up MDE problem cb_cknn_mde = tg.MDE(cb_cknn_graph) cb_cknn_emb = cb_cknn_mde.embed(verbose=False, snapshot_every=1) # Add to adata adata.obsm['X_cb_cknn_MDE'] = cb_cknn_emb # Plot with scanpy sc.pl.embedding(adata, basis='cb_cknn_MDE', color=['leiden'], edges=False, size=5) Building topological graph... Topological graph extracted in = 0.776618 (sec) # Explore how data behaves during optimization cb_cknn_mde.play(savepath=wd + 'pbmc10k_cb_cknn_MDE.gif', marker_size=2, color_by=adata.obs['leiden'], axis_limits=[-4, 4], figsize_inches=(2,2)) HBox(children=(FloatProgress(value=0.0, max=301.0), HTML(value=''))) Now that we have computed our results, let's look how our tg TopOGraph looks like! tg TopoGraph object with 5942 samples and 1338 observations and: Diffusion basis fitted - .DiffBasis Continuous basis fitted - .ContBasis Multiscale Diffusion Maps fitted - .MSDiffMap Continuous Laplacian Eigenmaps fitted - .CLapMap Diffusion graph fitted - .DiffGraph Continuous graph fitted - .CknnGraph Active basis: continuous basis. Active graph: cknn graph. Comparing the obtained embeddings Cool! Now let's compare how our embeddings look like, and how the default clustering and some marker genes expressions are distributed. sc.pl.embedding(adata, basis='umap', color=['leiden', 'IL7R', 'CST3', 'MS4A1', 'NKG7','PPBP'], ncols=3) sc.pl.embedding(adata, basis='dbMAP', color=['leiden', 'IL7R', 'CST3', 'MS4A1', 'NKG7','PPBP'], ncols=3) sc.pl.embedding(adata, basis='db_diff_MDE', color=['leiden', 'IL7R', 'CST3', 'MS4A1', 'NKG7','PPBP'], ncols=3) sc.pl.embedding(adata, basis='db_cknn_MDE', color=['leiden', 'IL7R', 'CST3', 'MS4A1', 'NKG7','PPBP'], ncols=3) sc.pl.embedding(adata, basis='cb_diff_MDE', color=['leiden', 'IL7R', 'CST3', 'MS4A1', 'NKG7','PPBP'], ncols=3) sc.pl.embedding(adata, basis='cb_cknn_MDE', color=['leiden', 'IL7R', 'CST3', 'MS4A1', 'NKG7','PPBP'], ncols=3) Re-clustering cells without the PCA basis Because of the computational expense involved in single-cell analysis, it is common-place for analysis toolkits to perform PCA as a preprocessing step. Neighborhoods (and clusters) are then computed from the PCA basis, which can be misleading when dealing with high-dimensional, non-linear data. Here, we'll show how providing Scanpy with TopOMetry learned metrics can improve its clustering. For demonstration purposes, we'll use scanpy built-in leiden algorithm. First, let's add TopOMetry metrics to the `AnnData` object containing our single-cell data. # Recall what's inside our `tg` TopOGraph tg TopoGraph object with 5942 samples and 1338 observations and: Diffusion basis fitted - .DiffBasis Continuous basis fitted - .ContBasis Multiscale Diffusion Maps fitted - .MSDiffMap Continuous Laplacian Eigenmaps fitted - .CLapMap Diffusion graph fitted - .DiffGraph Continuous graph fitted - .CknnGraph Active basis: continuous basis. Active graph: cknn graph. # Store the PCA-derived clusters adata.obs['pca_leiden'] = adata.obs['leiden'] # Compute clusters with the leiden algorithm using topological basis sc.pp.neighbors(adata, use_rep='X_ms_diff_map') # MSDiffMap sc.tl.leiden(adata, key_added='db_leiden') sc.pp.neighbors(adata, use_rep='X_c_lapmap') #CLapMap sc.tl.leiden(adata, key_added='cb_leiden') computing neighbors finished: added to `.uns['neighbors']` `.obsp['distances']`, distances for each pair of neighbors `.obsp['connectivities']`, weighted adjacency matrix (0:00:00) running Leiden clustering finished: found 21 clusters and added 'db_leiden', the cluster labels (adata.obs, categorical) (0:00:00) computing neighbors finished: added to `.uns['neighbors']` `.obsp['distances']`, distances for each pair of neighbors `.obsp['connectivities']`, weighted adjacency matrix (0:00:00) running Leiden clustering finished: found 33 clusters and added 'cb_leiden', the cluster labels (adata.obs, categorical) (0:00:00) sc.pl.embedding(adata, basis='umap', color=['pca_leiden', 'db_leiden', 'cb_leiden'], legend_loc=None, ncols=3) sc.pl.embedding(adata, basis='dbMAP', color=['pca_leiden', 'db_leiden', 'cb_leiden'], legend_loc=None, ncols=3) sc.pl.embedding(adata, basis='db_diff_MDE', color=['pca_leiden', 'db_leiden', 'cb_leiden'], legend_loc=None, ncols=3) sc.pl.embedding(adata, basis='db_cknn_MDE', color=['pca_leiden', 'db_leiden', 'cb_leiden'], legend_loc=None, ncols=3) sc.pl.embedding(adata, basis='cb_diff_MDE', color=['pca_leiden', 'db_leiden', 'cb_leiden'], legend_loc=None, ncols=3) sc.pl.embedding(adata, basis='cb_cknn_MDE', color=['pca_leiden', 'db_leiden', 'cb_leiden'], legend_loc=None, ncols=3) That's it for this tutorial! I hope you enjoyed and that TopOMetry might be useful for you!","title":"Single-cell analysis"},{"location":"TopOMetry_Intro_pbmc_10k/#analyzing-pbmcs-single-cell-data-with-topometry","text":"TopOMetry is intended for the topological analysis and visualization of high-dimensional data. In this notebook, I showcase how it can be used for the analysis of single-cell data, a challenging data modality which has been pushing representation learning to new frontiers. We'll be using a dataset containing around 6,000 individually RNA sequenced cells, consisting of peripherical blood mononuclear cells extracted from a healthy donor. To assist us throghout this analysis, we'll be using scanpy , a scalable toolkit for analyzing single-cell RNA sequencing data. In particular, we'll use the anndata framework for storing single-cell data and its plotting API for some plotting.","title":"Analyzing PBMCs single-cell data with TopOMetry"},{"location":"TopOMetry_Intro_pbmc_10k/#install-and-load-libraries","text":"# Install pre-requisites and scanpy (pre-requisites for scanpy are python-igraph and leidenalg) #!pip install nmslib annoy scipy scanpy numba kneed pymde python-igraph leidenalg scanpy # Install pre-release of TopOMetry #!pip install -i https://test.pypi.org/simple/ topo import numpy as np import pandas as pd import scanpy as sc import pymde import topo as tp /usr/local/lib/python3.8/dist-packages/dask/config.py:161: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details. data = yaml.load(f.read()) or {} These are some scanpy helper functions: sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3) sc.logging.print_header() sc.settings.set_figure_params(dpi=60, dpi_save=300, facecolor='white') scanpy==1.6.0 anndata==0.7.6 umap==0.4.6 numpy==1.18.5 scipy==1.5.3 pandas==1.1.3 scikit-learn==0.23.1 statsmodels==0.11.1 python-igraph==0.9.1 louvain==0.7.0 leidenalg==0.8.1","title":"Install and load libraries"},{"location":"TopOMetry_Intro_pbmc_10k/#load-data-and-qc","text":"Download data from 10X genomics website : !mkdir data !wget https://cf.10xgenomics.com/samples/cell-exp/6.0.0/SC3_v3_NextGem_DI_CellPlex_Human_PBMC_10K_PBMCs_human_1/SC3_v3_NextGem_DI_CellPlex_Human_PBMC_10K_PBMCs_human_1_count_sample_feature_bc_matrix.h5 -O data/SC3_v3_NextGem_DI_CellPlex_Human_PBMC_10K_PBMCs_human_1_count_sample_feature_bc_matrix.h5 !mkdir write mkdir: cannot create directory \u2018data\u2019: File exists --2021-05-11 21:43:42-- https://cf.10xgenomics.com/samples/cell-exp/6.0.0/SC3_v3_NextGem_DI_CellPlex_Human_PBMC_10K_PBMCs_human_1/SC3_v3_NextGem_DI_CellPlex_Human_PBMC_10K_PBMCs_human_1_count_sample_feature_bc_matrix.h5 Resolving cf.10xgenomics.com (cf.10xgenomics.com)... 2606:4700::6812:1ad, 2606:4700::6812:ad, 104.18.0.173, ... Connecting to cf.10xgenomics.com (cf.10xgenomics.com)|2606:4700::6812:1ad|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 15239045 (15M) [binary/octet-stream] Saving to: \u2018data/SC3_v3_NextGem_DI_CellPlex_Human_PBMC_10K_PBMCs_human_1_count_sample_feature_bc_matrix.h5\u2019 data/SC3_v3_NextGem 100%[===================>] 14.53M 6.05MB/s in 2.4s 2021-05-11 21:43:45 (6.05 MB/s) - \u2018data/SC3_v3_NextGem_DI_CellPlex_Human_PBMC_10K_PBMCs_human_1_count_sample_feature_bc_matrix.h5\u2019 saved [15239045/15239045] mkdir: cannot create directory \u2018write\u2019: File exists wd = '/home/davi/Documents/Bioinfo/pheno_tutorials/pbmc10k/' results_file = 'write/pbmc10k.h5ad' # the file that will store the analysis results Read in the data with Scanpy: adata = sc.read_10x_h5( 'data/SC3_v3_NextGem_DI_CellPlex_Human_PBMC_10K_PBMCs_human_1_count_sample_feature_bc_matrix.h5') adata.var_names_make_unique() adata reading data/SC3_v3_NextGem_DI_CellPlex_Human_PBMC_10K_PBMCs_human_1_count_sample_feature_bc_matrix.h5 Variable names are not unique. To make them unique, call `.var_names_make_unique`. (0:00:00) Variable names are not unique. To make them unique, call `.var_names_make_unique`. AnnData object with n_obs \u00d7 n_vars = 6437 \u00d7 36601 var: 'gene_ids', 'feature_types', 'genome' This data contains 6,437 cells with 36,601 sequenced genes each. A great deal of this is just experimental noise, so we need to perform some quality control. Here, we'll do the default quality-control analysis: # Default QC sc.pp.filter_cells(adata, min_genes=200) sc.pp.filter_genes(adata, min_cells=3) adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes as 'mt' sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True) sc.pl.violin(adata, ['n_genes', 'total_counts', 'pct_counts_mt'], jitter=0.4, multi_panel=True) filtered out 13 cells that have less than 200 genes expressed filtered out 17855 genes that are detected in less than 3 cells ... storing 'feature_types' as categorical ... storing 'genome' as categorical Actually perform some filtering: adata = adata[adata.obs.n_genes < 3000, :] adata = adata[adata.obs.total_counts < 10000, :] adata = adata[adata.obs.pct_counts_mt < 20, :] adata View of AnnData object with n_obs \u00d7 n_vars = 5942 \u00d7 18746 obs: 'n_genes', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt' var: 'gene_ids', 'feature_types', 'genome', 'n_cells', 'mt', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts'","title":"Load data and QC"},{"location":"TopOMetry_Intro_pbmc_10k/#default-scanpy-workflow-pca-based","text":"Here, for brevity sake, we'll perform the default scanpy workflow. For more information about the analysis of single-cell data and proposed workflows, we direct the interested user to Scanpy tutorials and this excellent best-practices review . In short, this involves normalizing the library size of each sequenced cell by the total number of detected mRNA molecules, then logarithmizing it. Next, genes with high expression mean and high dispersion are selected as highly-variable genes, and stored for downstream analysis. Data is then scaled and mean-centered. The default workflow further involves computing PCA, and then using top principal components to compute a neighborhood graph. From this graph, it is possible to cluster cells with the leiden algorithm, and to obtain lower-dimensional embeddings with UMAP. sc.pp.normalize_total(adata, target_sum=1e4) sc.pp.log1p(adata) sc.pp.highly_variable_genes(adata, min_mean=0.125, max_mean=8, min_disp=0.3) adata.raw = adata adata = adata[:, adata.var.highly_variable] sc.pp.scale(adata, max_value=10) sc.tl.pca(adata) sc.pp.neighbors(adata) sc.tl.leiden(adata) sc.tl.umap(adata) sc.pl.embedding(adata, basis='umap', color=['leiden'], size=5) normalizing counts per cell finished (0:00:00) extracting highly variable genes /home/davi/.local/lib/python3.8/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata) finished (0:00:00) --> added 'highly_variable', boolean vector (adata.var) 'means', float vector (adata.var) 'dispersions', float vector (adata.var) 'dispersions_norm', float vector (adata.var) ... as `zero_center=True`, sparse input is densified and may lead to large memory consumption computing PCA on highly variable genes with n_comps=50 /home/davi/.local/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py:806: UserWarning: Revieved a view of an AnnData. Making a copy. view_to_actual(adata) finished (0:00:01) computing neighbors using 'X_pca' with n_pcs = 50 finished: added to `.uns['neighbors']` `.obsp['distances']`, distances for each pair of neighbors `.obsp['connectivities']`, weighted adjacency matrix (0:00:02) running Leiden clustering finished: found 13 clusters and added 'leiden', the cluster labels (adata.obs, categorical) (0:00:00) computing UMAP finished: added 'X_umap', UMAP coordinates (adata.obsm) (0:00:07) This looks neat. Let's see how it compares to the TopOMetry workflow.","title":"Default Scanpy workflow (PCA-based)"},{"location":"TopOMetry_Intro_pbmc_10k/#topological-analysis-with-topometry","text":"Now that we have concluded the default Scanpy analysis, let's perform some topological analyses with TopOMetry. For this, we will start up a TopOGraph object with some data, and populate its slots as we proceed with the analysis. These 4 steps will be followed: Learn topological metrics - this can be done with diffusion harmonics or continuous k-nearest-neighbors (ckNN) Build a topological basis - this can be done with multiscale diffusion maps or a continuous Laplacian Eigenmap Extract a topological graph - again, this can be done with diffusion harmonics or ckNN Optimize a topological layout - this can be done with a Minimum Distortion Embedding or a Manifold Approximation and Projection For now, let's start with the default TopOMetry analysis - using diffusion harmonics, multiscale maps and graphs.","title":"Topological analysis with TopOMetry"},{"location":"TopOMetry_Intro_pbmc_10k/#fit-a-topological-basis","text":"# Convert the data - we'll use a csr_matrix, for speed from scipy.sparse import csr_matrix data = csr_matrix(adata.X) # scanpy stores the gene expression data at adata.X # Start up the TopOGraph tg = tp.models.TopOGraph(n_jobs=12, # set this to the highest number of available threads plot_spectrum=True) tg.fit(data) Building topological basis... Topological basis fitted with diffusion mappings in 39.156711 (sec) TopoGraph object with 5942 samples and 1338 observations and: Diffusion basis fitted - .DiffBasis Multiscale Diffusion Maps fitted - .MSDiffMap Active basis: diffusion basis. Active graph: diff graph. TopOMetry allows the user to visualize the dataset diffusion eigenspectrum when building multiscale diffusion maps. It automatically estimates the dataset dimensionality and selects and adequate number of diffusion components to scale and store for downstream analysis at TopoGraph.MSDiffMap . Users are also allowed to rescale the diffusion components to another number of components, if they want to obtain less or further detail on their dataset. As rule of thumb, more components lead to more detailed analyses, but can result in very disconnected embeddings. After re-scaling our multiscale diffusion maps, we'll add it to our AnnData object. # Convenience function to re-scale to user's chosen number of components tg.MSDiffMap = tg.DiffBasis.rescale(n_eigs=50) # Add the multiscale diffusion maps to adata as a lower-dimensional representation adata.obsm['X_ms_diff_map'] = tg.MSDiffMap","title":"Fit a topological basis"},{"location":"TopOMetry_Intro_pbmc_10k/#fit-the-topological-graph","text":"Let's take a look at our TopOGraph object: tg TopoGraph object with 5942 samples and 1338 observations and: Diffusion basis fitted - .DiffBasis Multiscale Diffusion Maps fitted - .MSDiffMap Active basis: diffusion basis. Active graph: diff graph. After learning a topological basis, extracting an associated topological graph from the active basis is as simple as running TopOGraph.transform() . Note that the graph will be computed as per the TopOGraph.graph parameter (default diff , diffusion harmonics), on the specified TopOGraph.basis parameter (the active basis). db_diff_graph = tg.transform() Building topological graph... Topological graph extracted in = 0.235041 (sec)","title":"Fit the topological graph"},{"location":"TopOMetry_Intro_pbmc_10k/#optimize-the-topological-graph-layout","text":"Now that we have learned a topological graph from our topological basis, we need to optimize its layout in order to visualize it. TopOMetry offers two options for that task: MAP - Manifold Approximation and Projection, a generalized version of UMAP that does not hold the uniform distribution assumption. It's goal is to minimize the divergence between a given basis and a topological graph. MDE - Minimum Distortion Embedding, a recently proposed dimensionality reduction method that generalized the problem of effective data visualization through convex optimization. Naturally, you may perform visualization with any other method, such as t-SNE, PHATE or autoencoders, and still take advantage of TopOMetry by simply using the topological basis as input instead of raw data. Alternatively, if your chosen method takes as input a matrix of similarities, you can use the topological graph. First, we'll perform layout optimization with the MAP approach: # Minimize divergence between the diffusion basis and diffusion graph emb_dbmap, aux = tg.MAP(tg.MSDiffMap, db_diff_graph) # Add learned embedding to adata adata.obsm['X_dbMAP'] = emb_dbmap # Plot with Scanpy sc.pl.embedding(adata, basis='dbMAP', color=['leiden'], edges=False, size=5) Fuzzy layout optimization embedding in = 6.311060 (sec) Next, we'll perform the layout optimization with pyMDE. This involves building a pyMDE graph from our topological graph (so to be compatible with pytorch), setting up an MDE problem, and effectively optimizing the embedding. TopOMetry has pyMDE functions built-in in the topo.lt module. A separate tutorial exploring the wide array of options involved in MDE optimization is in the making. For now, let's run MDE with the default parameters: # Layout optimization with Minimum Distortion Embedding # Set up MDE problem db_diff_mde = tg.MDE(db_diff_graph, constraint=None) db_diff_emb = db_diff_mde.embed(verbose=False, snapshot_every=1) # Add to adata adata.obsm['X_db_diff_MDE'] = db_diff_emb # Plot with scanpy sc.pl.embedding(adata, basis='db_diff_MDE', color=['leiden'], edges=False, size=5) pyMDE allows us to visualize how data looks like during the optimization process: # Explore how data behaves during optimization db_diff_mde.play(savepath= wd + 'pbmc10k_db_diff_MDE.gif',marker_size=2, color_by=adata.obs['leiden'], figsize_inches=(2,2)) HBox(children=(FloatProgress(value=0.0, max=301.0), HTML(value=''))) Let's now see what we have got inside the tg object storing our TopOGraph: tg TopoGraph object with 5942 samples and 1338 observations and: Diffusion basis fitted - .DiffBasis Multiscale Diffusion Maps fitted - .MSDiffMap Diffusion graph fitted - .DiffGraph Active basis: diffusion basis. Active graph: diff graph.","title":"Optimize the topological graph layout"},{"location":"TopOMetry_Intro_pbmc_10k/#exploring-alternative-topologies-with-topometry","text":"Now that we have explored some of data topology with the default TopOMetry analysis, we can also explore it under continuous nearest-neighbors topologies. We will explore the following combinations: 1 - Diffusion basis, diffusion graph (default, as we did above) 2 - Diffusion basis, continuous graph 3 - Continuous basis, diffusion graph 4 - Continuous basis, continuous graph Although all these models render similar results, they give you, the end user, flexibility to find the better suited combination for your data. From our experience, the default combination tends to have more robust and significative results throghout a myriad of datasets, but you have a choice :) Let's now try combination (2). Using our active basis, let's change our active graph model to cknn. tg.graph = 'cknn' db_cknn_graph = tg.transform() Building topological graph... Topological graph extracted in = 0.915521 (sec) Again, let's perform layout optimization with Minimum Distortion Embedding # Set up MDE problem db_cknn_mde = tg.MDE(db_cknn_graph) db_cknn_emb = db_cknn_mde.embed(verbose=False, snapshot_every=1) # Add to adata adata.obsm['X_db_cknn_MDE'] = db_cknn_emb # Plot with scanpy sc.pl.embedding(adata, basis='db_cknn_MDE', color=['leiden'], edges=False, size=5) # Explore how data behaves during optimization db_cknn_mde.play(savepath= wd + 'pbmc10k_db_cknn_MDE.gif', marker_size=2, color_by=adata.obs['leiden'], axis_limits=[-5, 5], figsize_inches=(2,2)) HBox(children=(FloatProgress(value=0.0, max=301.0), HTML(value=''))) As we can see, we still get more or less the same clusters distribution. However, we also see that this model is more sensitive to outliers, represented by the sparsely distributed cells without any relation to any cluster. Now, let's explore how a continuous basis looks like. For that, we'll need to change the basis argument of our tg TopOGraph object, and fit the model once again. The previously computed diffusion basis is not lost in this process. Note that when computing a continuous basis, a continuous version of Laplacian Eigenmaps is used to embed the metric into latent dimensions. We'll start with combination (3), using a continuous basis and a diffusion graph. tg.basis = 'continuous' tg.fit(data) tg.graph = 'diff' cb_diff_graph = tg.transform() # Add the continuous Laplacian Eigenmaps to adata adata.obsm['X_c_lapmap'] = tg.CLapMap Building topological basis... /usr/local/lib/python3.8/dist-packages/sklearn/manifold/_spectral_embedding.py:236: UserWarning: Graph is not fully connected, spectral embedding may not work as expected. warnings.warn(\"Graph is not fully connected, spectral embedding\" Topological basis fitted with continuous mappings in 116.046612 (sec) Building topological graph... Topological graph extracted in = 0.289549 (sec) # Set up MDE problem cb_diff_mde = tg.MDE(cb_diff_graph) cb_diff_emb = cb_diff_mde.embed(verbose=False, snapshot_every=1) # Add to adata adata.obsm['X_cb_diff_MDE'] = cb_diff_emb # Plot with scanpy sc.pl.embedding(adata, basis='cb_diff_MDE', color=['leiden'], edges=False, size=5) # Explore how data behaves during optimization cb_diff_mde.play(savepath=wd + 'pbmc10k_cb_diff_MDE.gif', marker_size=2, color_by=adata.obs['leiden'], axis_limits=[-3, 3], figsize_inches=(2,2)) HBox(children=(FloatProgress(value=0.0, max=301.0), HTML(value=''))) Now let's go through with combination (4), using a continuous basis and a continuous graph. Because we have already fitted the continuous basis in our tg object, we only need to compute the graph. tg.graph = 'cknn' cb_cknn_graph = tg.transform() # Set up MDE problem cb_cknn_mde = tg.MDE(cb_cknn_graph) cb_cknn_emb = cb_cknn_mde.embed(verbose=False, snapshot_every=1) # Add to adata adata.obsm['X_cb_cknn_MDE'] = cb_cknn_emb # Plot with scanpy sc.pl.embedding(adata, basis='cb_cknn_MDE', color=['leiden'], edges=False, size=5) Building topological graph... Topological graph extracted in = 0.776618 (sec) # Explore how data behaves during optimization cb_cknn_mde.play(savepath=wd + 'pbmc10k_cb_cknn_MDE.gif', marker_size=2, color_by=adata.obs['leiden'], axis_limits=[-4, 4], figsize_inches=(2,2)) HBox(children=(FloatProgress(value=0.0, max=301.0), HTML(value=''))) Now that we have computed our results, let's look how our tg TopOGraph looks like! tg TopoGraph object with 5942 samples and 1338 observations and: Diffusion basis fitted - .DiffBasis Continuous basis fitted - .ContBasis Multiscale Diffusion Maps fitted - .MSDiffMap Continuous Laplacian Eigenmaps fitted - .CLapMap Diffusion graph fitted - .DiffGraph Continuous graph fitted - .CknnGraph Active basis: continuous basis. Active graph: cknn graph.","title":"Exploring alternative topologies with TopOMetry"},{"location":"TopOMetry_Intro_pbmc_10k/#comparing-the-obtained-embeddings","text":"Cool! Now let's compare how our embeddings look like, and how the default clustering and some marker genes expressions are distributed. sc.pl.embedding(adata, basis='umap', color=['leiden', 'IL7R', 'CST3', 'MS4A1', 'NKG7','PPBP'], ncols=3) sc.pl.embedding(adata, basis='dbMAP', color=['leiden', 'IL7R', 'CST3', 'MS4A1', 'NKG7','PPBP'], ncols=3) sc.pl.embedding(adata, basis='db_diff_MDE', color=['leiden', 'IL7R', 'CST3', 'MS4A1', 'NKG7','PPBP'], ncols=3) sc.pl.embedding(adata, basis='db_cknn_MDE', color=['leiden', 'IL7R', 'CST3', 'MS4A1', 'NKG7','PPBP'], ncols=3) sc.pl.embedding(adata, basis='cb_diff_MDE', color=['leiden', 'IL7R', 'CST3', 'MS4A1', 'NKG7','PPBP'], ncols=3) sc.pl.embedding(adata, basis='cb_cknn_MDE', color=['leiden', 'IL7R', 'CST3', 'MS4A1', 'NKG7','PPBP'], ncols=3)","title":"Comparing the obtained embeddings"},{"location":"TopOMetry_Intro_pbmc_10k/#re-clustering-cells-without-the-pca-basis","text":"Because of the computational expense involved in single-cell analysis, it is common-place for analysis toolkits to perform PCA as a preprocessing step. Neighborhoods (and clusters) are then computed from the PCA basis, which can be misleading when dealing with high-dimensional, non-linear data. Here, we'll show how providing Scanpy with TopOMetry learned metrics can improve its clustering. For demonstration purposes, we'll use scanpy built-in leiden algorithm. First, let's add TopOMetry metrics to the `AnnData` object containing our single-cell data. # Recall what's inside our `tg` TopOGraph tg TopoGraph object with 5942 samples and 1338 observations and: Diffusion basis fitted - .DiffBasis Continuous basis fitted - .ContBasis Multiscale Diffusion Maps fitted - .MSDiffMap Continuous Laplacian Eigenmaps fitted - .CLapMap Diffusion graph fitted - .DiffGraph Continuous graph fitted - .CknnGraph Active basis: continuous basis. Active graph: cknn graph. # Store the PCA-derived clusters adata.obs['pca_leiden'] = adata.obs['leiden'] # Compute clusters with the leiden algorithm using topological basis sc.pp.neighbors(adata, use_rep='X_ms_diff_map') # MSDiffMap sc.tl.leiden(adata, key_added='db_leiden') sc.pp.neighbors(adata, use_rep='X_c_lapmap') #CLapMap sc.tl.leiden(adata, key_added='cb_leiden') computing neighbors finished: added to `.uns['neighbors']` `.obsp['distances']`, distances for each pair of neighbors `.obsp['connectivities']`, weighted adjacency matrix (0:00:00) running Leiden clustering finished: found 21 clusters and added 'db_leiden', the cluster labels (adata.obs, categorical) (0:00:00) computing neighbors finished: added to `.uns['neighbors']` `.obsp['distances']`, distances for each pair of neighbors `.obsp['connectivities']`, weighted adjacency matrix (0:00:00) running Leiden clustering finished: found 33 clusters and added 'cb_leiden', the cluster labels (adata.obs, categorical) (0:00:00) sc.pl.embedding(adata, basis='umap', color=['pca_leiden', 'db_leiden', 'cb_leiden'], legend_loc=None, ncols=3) sc.pl.embedding(adata, basis='dbMAP', color=['pca_leiden', 'db_leiden', 'cb_leiden'], legend_loc=None, ncols=3) sc.pl.embedding(adata, basis='db_diff_MDE', color=['pca_leiden', 'db_leiden', 'cb_leiden'], legend_loc=None, ncols=3) sc.pl.embedding(adata, basis='db_cknn_MDE', color=['pca_leiden', 'db_leiden', 'cb_leiden'], legend_loc=None, ncols=3) sc.pl.embedding(adata, basis='cb_diff_MDE', color=['pca_leiden', 'db_leiden', 'cb_leiden'], legend_loc=None, ncols=3) sc.pl.embedding(adata, basis='cb_cknn_MDE', color=['pca_leiden', 'db_leiden', 'cb_leiden'], legend_loc=None, ncols=3) That's it for this tutorial! I hope you enjoyed and that TopOMetry might be useful for you!","title":"Re-clustering cells without the PCA basis"},{"location":"diffusor/","text":"Documentation for Diffusor topo.tpgraph.diffusion.Diffusor Sklearn-compatible estimator for using fast anisotropic diffusion with an adaptive neighborhood search algorithm. The Diffusion Maps algorithm was initially proposed by Coifman et al in 2005, and was augmented by the work of many. This implementation aggregates recent advances in diffusion harmonics, and innovates only by implementing an adaptively decaying kernel (the rate of decay is dependent on neighborhood density) and an adaptive neighborhood estimation approach. Parameters n_eigs : int (optional, default 50) Number of diffusion components to compute. This number can be iterated to get different views from data at distinct spectral resolution. use_eigs : int or str (optional, default 'knee') Number of eigenvectors to use. If 'max', expands to the maximum number of positive eigenvalues (reach of numerical precision), else to the maximum amount of computed components. If 'knee', uses Kneedle to find an optimal cutoff point, and expands it by expansion . If 'comp_gap', tries to find a discrete eigengap from the computation process. n_neighbors : int (optional, default 10) Number of k-nearest-neighbors to compute. The adaptive kernel will normalize distances by each cell distance of its median neighbor. Nonetheless, this hyperparameter remains as an user input regarding the minimal sample neighborhood resolution that drives the computation of the diffusion metrics. For practical purposes, the minimum amount of samples one would expect to constitute a neighborhood of its own. Increasing k can generate more globally-comprehensive metrics and maps, to a certain extend, however at the expense of fine-grained resolution. More generally, consider this a calculus discretization threshold. ann : bool (optional, default True) Whether to use approximate nearest neighbors for graph construction with NMSLib. If False , uses sklearn default implementation. metric : str (optional, default 'cosine') Distance metric for building an approximate kNN graph. Defaults to 'euclidean'. Users are encouraged to explore different metrics, such as 'cosine' and 'jaccard'. The 'hamming' and 'jaccard' distances are also available for string vectors. Accepted metrics include NMSLib metrics and sklearn metrics. Some examples are: -'sqeuclidean' -'euclidean' -'l1' -'lp' - requires setting the parameter p -'cosine' -'angular' -'negdotprod' -'levenshtein' -'hamming' -'jaccard' -'jansen-shan' p : int or float (optional, default 11/16 ) P for the Lp metric, when metric='lp' . Can be fractional. The default 11/16 approximates an astroid norm with some computational efficiency (2^n bases are less painstakinly slow to compute). See https://en.wikipedia.org/wiki/Lp_space for some context. transitions : bool (optional, default False) Whether to estimate the diffusion transitions graph. If True , maps a basis encoding neighborhood transitions probability during eigendecomposition. If 'False' (default), maps the diffusion kernel. alpha : int or float (optional, default 1) Alpha in the diffusion maps literature. Controls how much the results are biased by data distribution. Defaults to 1, which is suitable for normalized data. kernel_use : str (optional, default 'decay_adaptive') Which type of kernel to use. There are four implemented, considering the adaptive decay and the neighborhood expansion, written as 'simple', 'decay', 'simple_adaptive' and 'decay_adaptive'. The first, 'simple' , is a locally-adaptive kernel similar to that proposed by Nadler et al.(https://doi.org/10.1016/j.acha.2005.07.004) and implemented in Setty et al. (https://doi.org/10.1038/s41587-019-0068-4). The 'decay' option applies an adaptive decay rate, but no neighborhood expansion. Those, followed by '_adaptive', apply the neighborhood expansion process. The default and recommended is 'decay_adaptive'. The neighborhood expansion can impact runtime, although this is not usually expressive for datasets under 10e6 samples. transitions : bool (optional, default False) Whether to decompose the transition graph when transforming. norm : bool (optional, default True) Whether to normalize the kernel transition probabilities to approximate the LPO. eigengap : bool (optional, default True) Whether to expand the eigendecomposition a bit and stop if eigenvalues sign shift (limit of float64). Used to guarantee numerical stability. n_jobs : int (optional, default 4) Number of threads to use in calculations. Defaults to 4 for safety, but performance scales dramatically when using more threads. plot_spectrum : bool (optional, default False) Whether to plot the spectrum decay analysis. verbose : bool (optional, default False) Controls verbosity. cache : bool (optional, default True) Whether to cache nearest-neighbors (before fit) and to store diffusion matrices after mapping (before transform). Example import numpy as np from sklearn.datasets import load_digits from scipy.sparse import csr_matrix from topo.tpgraph.diffusion import Diffusor digits = load_digits() data = csr_matrix(digits) diff = Diffusor().fit(data) msdiffmap = diff.transform(data) fit ( self , X ) Fits an adaptive anisotropic diffusion kernel to the data. Parameters X : input data. Takes in numpy arrays and scipy csr sparse matrices. Use with sparse data for top performance. You can adjust a series of parameters that can make the process faster and more informational depending on your dataset. Returns Diffusor object with kernel Diffusor.K and the transition potencial Diffusor.T . Source code in topo/tpgraph/diffusion.py def fit ( self , X ): \"\"\" Fits an adaptive anisotropic diffusion kernel to the data. Parameters ---------- X : input data. Takes in numpy arrays and scipy csr sparse matrices. Use with sparse data for top performance. You can adjust a series of parameters that can make the process faster and more informational depending on your dataset. Returns ------- Diffusor object with kernel Diffusor.K and the transition potencial Diffusor.T . \"\"\" data = X self . start_time = time . time () self . N = data . shape [ 0 ] self . M = data . shape [ 1 ] if self . kernel_use not in [ 'simple' , 'simple_adaptive' , 'decay' , 'decay_adaptive' ]: raise Exception ( 'Kernel must be either \\' simple \\' , \\' simple_adaptive \\' , \\' decay \\' or \\' decay_adaptive \\' .' ) if self . ann : # Construct an approximate k-nearest-neighbors graph anbrs = ann . NMSlibTransformer ( n_neighbors = self . n_neighbors , metric = self . metric , p = self . p , method = 'hnsw' , n_jobs = self . n_jobs , M = self . M , efC = self . efC , efS = self . efS , verbose = self . verbose ) . fit ( data ) knn = anbrs . transform ( data ) # X, y specific stds: Normalize by the distance of median nearest neighbor to account for neighborhood size. median_k = np . floor ( self . n_neighbors / 2 ) . astype ( np . int ) adap_sd = np . zeros ( self . N ) for i in np . arange ( len ( adap_sd )): adap_sd [ i ] = np . sort ( knn . data [ knn . indptr [ i ]: knn . indptr [ i + 1 ]])[ median_k - 1 ] else : if self . metric == 'lp' : raise Exception ( 'Generalized Lp distances are available only with `ann` set to True.' ) # Construct a k-nearest-neighbors graph nbrs = NearestNeighbors ( n_neighbors = int ( self . n_neighbors ), metric = self . metric , n_jobs = self . n_jobs ) . fit ( data ) knn = nbrs . kneighbors_graph ( data , mode = 'distance' ) # X, y specific stds: Normalize by the distance of median nearest neighbor to account for neighborhood size. median_k = np . floor ( self . n_neighbors / 2 ) . astype ( np . int ) adap_sd = np . zeros ( self . N ) for i in np . arange ( len ( adap_sd )): adap_sd [ i ] = np . sort ( knn . data [ knn . indptr [ i ]: knn . indptr [ i + 1 ]])[ median_k - 1 ] # Distance metrics x , y , dists = find ( knn ) # k-nearest-neighbor distances if self . cache : self . dists = knn self . adap_sd = adap_sd # Neighborhood graph expansion # define decay as sample's pseudomedian k-nearest-neighbor pm = np . interp ( adap_sd , ( adap_sd . min (), adap_sd . max ()), ( 2 , self . n_neighbors )) self . omega = pm # adaptive neighborhood size if self . kernel_use == 'simple_adaptive' or self . kernel_use == 'decay_adaptive' : self . new_k = int ( self . n_neighbors + ( self . n_neighbors - pm . max ())) # increase neighbor search: anbrs_new = ann . NMSlibTransformer ( n_neighbors = self . new_k , metric = self . metric , method = 'hnsw' , n_jobs = self . n_jobs , p = self . p , M = self . M , efC = self . efC , efS = self . efS ) . fit ( data ) knn_new = anbrs_new . transform ( data ) x_new , y_new , dists_new = find ( knn_new ) # adaptive neighborhood size adap_nbr = np . zeros ( self . N ) for i in np . arange ( len ( adap_nbr )): adap_k = int ( np . floor ( pm [ i ])) adap_nbr [ i ] = np . sort ( knn_new . data [ knn_new . indptr [ i ]: knn_new . indptr [ i + 1 ]])[ adap_k - 1 ] if self . cache : self . dists_new = knn_new self . adap_nbr_sd = adap_nbr if self . kernel_use == 'simple' : # X, y specific stds dists = dists / ( adap_sd [ x ] + 1e-10 ) # Normalize by the distance of median nearest neighbor W = csr_matrix (( np . exp ( - dists ), ( x , y )), shape = [ self . N , self . N ]) if self . kernel_use == 'simple_adaptive' : # X, y specific stds dists = dists_new / ( adap_nbr [ x_new ] + 1e-10 ) # Normalize by normalized contribution to neighborhood size. W = csr_matrix (( np . exp ( - dists ), ( x_new , y_new )), shape = [ self . N , self . N ]) if self . kernel_use == 'decay' : # X, y specific stds dists = ( dists / ( adap_sd [ x ] + 1e-10 )) ** np . power ( 2 , (( self . n_neighbors - pm [ x ]) / pm [ x ])) W = csr_matrix (( np . exp ( - dists ), ( x , y )), shape = [ self . N , self . N ]) if self . kernel_use == 'decay_adaptive' : # X, y specific stds dists = ( dists_new / ( adap_nbr [ x_new ] + 1e-10 )) ** np . power ( 2 , ((( int ( self . n_neighbors + ( self . n_neighbors - pm . max ()))) - pm [ x_new ]) / pm [ x_new ])) # Normalize by normalized contribution to neighborhood size. W = csr_matrix (( np . exp ( - dists ), ( x_new , y_new )), shape = [ self . N , self . N ]) # Kernel construction kernel = ( W + W . T ) / 2 self . K = kernel # handle nan, zeros self . K . data = np . where ( np . isnan ( self . K . data ), 1 , self . K . data ) # Diffusion through Markov chain D = np . ravel ( self . K . sum ( axis = 1 )) if self . alpha > 0 : # L_alpha D [ D != 0 ] = D [ D != 0 ] ** ( - self . alpha ) mat = csr_matrix (( D , ( range ( self . N ), range ( self . N ))), shape = [ self . N , self . N ]) kernel = mat . dot ( self . K ) . dot ( mat ) D = np . ravel ( kernel . sum ( axis = 1 )) D [ D != 0 ] = 1 / D [ D != 0 ] # Setting the diffusion operator if self . norm : self . K = kernel self . T = csr_matrix (( D , ( range ( self . N ), range ( self . N ))), shape = [ self . N , self . N ]) . dot ( self . K ) else : self . T = csr_matrix (( D , ( range ( self . N ), range ( self . N ))), shape = [ self . N , self . N ]) . dot ( self . K ) # Guarantee symmetry self . T = ( self . T + self . T . T ) / 2 self . T [( np . arange ( self . T . shape [ 0 ]), np . arange ( self . T . shape [ 0 ]))] = 0 end = time . time () if self . verbose : print ( 'Diffusion time = %f (sec), per sample= %f (sec), per sample adjusted for thread number= %f (sec)' % ( end - self . start_time , float ( end - self . start_time ) / self . N , self . n_jobs * float ( end - self . start_time ) / self . N )) return self ind_dist_grad ( self , data ) Utility function to get indices, distances and gradients from a multiscale diffusion map. Parameters data : Input data matrix (numpy array, pandas df, csr_matrix). !!! n_components \"int (optional, default None)\" Numper of components to map to prior to learning. Returns A tuple containing neighborhood indices, distances, gradient and a knn graph. Source code in topo/tpgraph/diffusion.py def ind_dist_grad ( self , data ): \"\"\" Utility function to get indices, distances and gradients from a multiscale diffusion map. Parameters ---------- data : Input data matrix (numpy array, pandas df, csr_matrix). n_components: int (optional, default None) Numper of components to map to prior to learning. Returns ------- A tuple containing neighborhood indices, distances, gradient and a knn graph. \"\"\" self . start_time = time . time () # Fit an optimal number of components based on the eigengap # Use user's or default initial guess multiplier = self . N // 10e4 # initial eigen value decomposition if self . transitions : D , V = eigs ( self . T , self . n_components , tol = 1e-4 , maxiter = self . N ) else : D , V = eigs ( self . K , self . n_components , tol = 1e-4 , maxiter = self . N ) D = np . real ( D ) V = np . real ( V ) inds = np . argsort ( D )[:: - 1 ] D = D [ inds ] V = V [:, inds ] # Normalize by the first diffusion component for i in range ( V . shape [ 1 ]): V [:, i ] = V [:, i ] / np . linalg . norm ( V [:, i ]) vals = np . array ( V ) pos = np . sum ( vals > 0 , axis = 0 ) residual = np . sum ( vals < 0 , axis = 0 ) if self . eigengap and len ( residual ) < 1 : #expand eigendecomposition target = self . n_components * multiplier while residual < 3 : print ( 'Eigengap not found for determined number of components. Expanding eigendecomposition to ' + str ( target ) + 'components.' ) if self . transitions : D , V = eigs ( self . T , target , tol = 1e-4 , maxiter = self . N ) else : D , V = eigs ( self . K , target , tol = 1e-4 , maxiter = self . N ) D = np . real ( D ) V = np . real ( V ) inds = np . argsort ( D )[:: - 1 ] D = D [ inds ] V = V [:, inds ] # Normalize by the first diffusion component for i in range ( V . shape [ 1 ]): V [:, i ] = V [:, i ] / np . linalg . norm ( V [:, i ]) vals = np . array ( V ) residual = np . sum ( vals < 0 , axis = 0 ) target = target * 2 if len ( residual ) > 30 : self . n_components = len ( pos ) + 15 # adapted eigen value decomposition if self . transitions : D , V = eigs ( self . T , self . n_components , tol = 1e-4 , maxiter = self . N ) else : D , V = eigs ( self . K , self . n_components , tol = 1e-4 , maxiter = self . N ) D = np . real ( D ) V = np . real ( V ) inds = np . argsort ( D )[:: - 1 ] D = D [ inds ] V = V [:, inds ] # Normalize by the first diffusion component for i in range ( V . shape [ 1 ]): V [:, i ] = V [:, i ] / np . linalg . norm ( V [:, i ]) # Create the results dictionary self . res = { 'EigenVectors' : V , 'EigenValues' : D } self . res [ 'EigenVectors' ] = pd . DataFrame ( self . res [ 'EigenVectors' ]) if not issparse ( data ): self . res [ 'EigenValues' ] = pd . Series ( self . res [ 'EigenValues' ]) self . res [ \"EigenValues\" ] = pd . Series ( self . res [ \"EigenValues\" ]) self . res [ 'MultiscaleComponents' ], self . kn , self . scaled_eigs = multiscale . multiscale ( self . res , n_eigs = self . use_eigs , verbose = self . verbose ) anbrs = ann . NMSlibTransformer ( n_neighbors = self . n_neighbors , metric = 'cosine' , method = 'hnsw' , n_jobs = self . n_jobs , M = self . M , efC = self . efC , efS = self . efS , dense = True , verbose = self . verbose ) . fit ( self . res [ 'MultiscaleComponents' ]) ind , dists , grad , graph = anbrs . ind_dist_grad ( self . res [ 'MultiscaleComponents' ]) end = time . time () print ( 'Diffusion time = %f (sec), per sample= %f (sec), per sample adjusted for thread number= %f (sec)' % ( end - self . start_time , float ( end - self . start_time ) / self . N , self . n_jobs * float ( end - self . start_time ) / self . N )) if self . plot_spectrum : self . spectrum_plot () return ind , dists , grad , graph rescale ( self , n_eigs = None ) Re-scale the multiscale procedure to a new number of components. Parameters self : Diffusor object. n_eigs : int. Number of diffusion components to multiscale. Returns np.ndarray containing the new multiscaled basis. Source code in topo/tpgraph/diffusion.py def rescale ( self , n_eigs = None ): \"\"\" Re-scale the multiscale procedure to a new number of components. Parameters ---------- self : Diffusor object. n_eigs : int. Number of diffusion components to multiscale. Returns ------- np.ndarray containing the new multiscaled basis. \"\"\" if n_eigs is None : n_eigs = self . n_components mms , self . kn , self . scaled_eigs = multiscale . multiscale ( self . res , n_eigs = n_eigs , verbose = self . verbose ) self . res [ 'MultiscaleComponents' ] = mms return mms spectrum_plot ( self , bla = None ) Plot the decay spectra. Parameters self : Diffusor object. bla : Here only for autodoc's sake. Returns A nice plot of the diffusion spectra. Source code in topo/tpgraph/diffusion.py def spectrum_plot ( self , bla = None ): \"\"\" Plot the decay spectra. Parameters ---------- self : Diffusor object. bla : Here only for autodoc's sake. Returns ------- A nice plot of the diffusion spectra. \"\"\" if self . kn is None : msc , self . kn , self . scaled_eigs = multiscale . multiscale ( self . res , n_eigs = self . use_eigs , verbose = self . verbose ) ax1 = plt . subplot ( 2 , 1 , 1 ) ax1 . set_title ( 'Spectrum decay and \\' knee \\' ( %i )' % int ( self . kn . knee )) ax1 . plot ( self . kn . x , self . kn . y , 'b' , label = 'data' ) ax1 . set_ylabel ( 'Eigenvalues' ) ax1 . set_xlabel ( 'Eigenvectors' ) ax1 . vlines ( self . kn . knee , plt . ylim ()[ 0 ], plt . ylim ()[ 1 ], linestyles = \"--\" , label = 'Knee' ) ax1 . legend ( loc = 'best' ) ax2 = plt . subplot ( 2 , 1 , 2 ) ax2 . set_title ( 'Curve analysis' ) ax2 . plot ( self . kn . x_normalized , self . kn . y_normalized , \"b\" , label = \"normalized\" ) ax2 . plot ( self . kn . x_difference , self . kn . y_difference , \"r\" , label = \"differential\" ) ax2 . set_xticks ( np . arange ( self . kn . x_normalized . min (), self . kn . x_normalized . max () + 0.1 , 0.1 ) ) ax2 . set_yticks ( np . arange ( self . kn . y_difference . min (), self . kn . y_normalized . max () + 0.1 , 0.1 ) ) ax2 . vlines ( self . kn . norm_knee , plt . ylim ()[ 0 ], plt . ylim ()[ 1 ], linestyles = \"--\" , label = \"Knee\" , ) ax2 . legend ( loc = \"best\" ) plt . tight_layout () plt . show () return plt transform ( self , X ) Fits the renormalized Laplacian approximating the Laplace Beltrami-Operator in a discrete eigendecomposition. Then multiscales the resulting components. Parameters X : input data. Takes in numpy arrays and scipy csr sparse matrices. Use with sparse data for top performance. You can adjust a series of parameters that can make the process faster and more informational depending on your dataset. Returns Diffusor.res['MultiscaleComponents']] Source code in topo/tpgraph/diffusion.py def transform ( self , X ): \"\"\" Fits the renormalized Laplacian approximating the Laplace Beltrami-Operator in a discrete eigendecomposition. Then multiscales the resulting components. Parameters ---------- X : input data. Takes in numpy arrays and scipy csr sparse matrices. Use with sparse data for top performance. You can adjust a series of parameters that can make the process faster and more informational depending on your dataset. Returns ------- ``Diffusor.res['MultiscaleComponents']]`` \"\"\" self . start_time = time . time () # Fit an optimal number of components based on the eigengap # Use user's or default initial guess # initial eigen value decomposition if self . transitions : D , V = eigs ( self . T , self . n_components , tol = 1e-4 , maxiter = ( self . N // 10 )) else : D , V = eigs ( self . K , self . n_components , tol = 1e-4 , maxiter = ( self . N // 10 )) D = np . real ( D ) V = np . real ( V ) inds = np . argsort ( D )[:: - 1 ] D = D [ inds ] V = V [:, inds ] # Normalize by the first diffusion component for i in range ( V . shape [ 1 ]): V [:, i ] = V [:, i ] / np . linalg . norm ( V [:, i ]) vals = np . array ( V ) pos = np . sum ( vals > 0 , axis = 0 ) residual = np . sum ( vals < 0 , axis = 0 ) if self . eigengap and len ( residual ) < 1 : #expand eigendecomposition target = self . n_components + 30 while residual < 3 : print ( 'Eigengap not found for determined number of components. Expanding eigendecomposition to ' + str ( target ) + 'components.' ) if self . transitions : D , V = eigs ( self . T , target , tol = 1e-4 , maxiter = ( self . N // 10 )) else : D , V = eigs ( self . K , target , tol = 1e-4 , maxiter = ( self . N // 10 )) D = np . real ( D ) V = np . real ( V ) inds = np . argsort ( D )[:: - 1 ] D = D [ inds ] V = V [:, inds ] # Normalize by the first diffusion component for i in range ( V . shape [ 1 ]): V [:, i ] = V [:, i ] / np . linalg . norm ( V [:, i ]) vals = np . array ( V ) residual = np . sum ( vals < 0 , axis = 0 ) pos = np . sum ( vals > 0 , axis = 0 ) target = int ( target * 1.6 ) if len ( residual ) > 30 : self . n_components = len ( pos ) + 5 # adapted eigen value decomposition if self . transitions : D , V = eigs ( self . T , self . n_components , tol = 1e-4 , maxiter = self . N ) else : D , V = eigs ( self . K , self . n_components , tol = 1e-4 , maxiter = self . N ) D = np . real ( D ) V = np . real ( V ) inds = np . argsort ( D )[:: - 1 ] D = D [ inds ] V = V [:, inds ] if not self . cache : del self . K del self . T # Create the results dictionary self . res = { 'EigenVectors' : V , 'EigenValues' : D } self . res [ 'EigenVectors' ] = pd . DataFrame ( self . res [ 'EigenVectors' ]) self . res [ \"EigenValues\" ] = pd . Series ( self . res [ \"EigenValues\" ]) self . res [ 'MultiscaleComponents' ], self . kn , self . scaled_eigs = multiscale . multiscale ( self . res , verbose = self . verbose ) end = time . time () if self . verbose : print ( 'Multiscale decomposition time = %f (sec), per sample= %f (sec), per sample adjusted for thread number= %f (sec)' % ( end - self . start_time , float ( end - self . start_time ) / self . N , self . n_jobs * float ( end - self . start_time ) / self . N )) if self . plot_spectrum : self . spectrum_plot () return self . res [ 'MultiscaleComponents' ]","title":"Diffusion harmonics"},{"location":"diffusor/#documentation-for-diffusor","text":"","title":"Documentation for Diffusor"},{"location":"diffusor/#topo.tpgraph.diffusion.Diffusor","text":"Sklearn-compatible estimator for using fast anisotropic diffusion with an adaptive neighborhood search algorithm. The Diffusion Maps algorithm was initially proposed by Coifman et al in 2005, and was augmented by the work of many. This implementation aggregates recent advances in diffusion harmonics, and innovates only by implementing an adaptively decaying kernel (the rate of decay is dependent on neighborhood density) and an adaptive neighborhood estimation approach.","title":"Diffusor"},{"location":"diffusor/#topo.tpgraph.diffusion.Diffusor--parameters","text":"n_eigs : int (optional, default 50) Number of diffusion components to compute. This number can be iterated to get different views from data at distinct spectral resolution. use_eigs : int or str (optional, default 'knee') Number of eigenvectors to use. If 'max', expands to the maximum number of positive eigenvalues (reach of numerical precision), else to the maximum amount of computed components. If 'knee', uses Kneedle to find an optimal cutoff point, and expands it by expansion . If 'comp_gap', tries to find a discrete eigengap from the computation process. n_neighbors : int (optional, default 10) Number of k-nearest-neighbors to compute. The adaptive kernel will normalize distances by each cell distance of its median neighbor. Nonetheless, this hyperparameter remains as an user input regarding the minimal sample neighborhood resolution that drives the computation of the diffusion metrics. For practical purposes, the minimum amount of samples one would expect to constitute a neighborhood of its own. Increasing k can generate more globally-comprehensive metrics and maps, to a certain extend, however at the expense of fine-grained resolution. More generally, consider this a calculus discretization threshold. ann : bool (optional, default True) Whether to use approximate nearest neighbors for graph construction with NMSLib. If False , uses sklearn default implementation. metric : str (optional, default 'cosine') Distance metric for building an approximate kNN graph. Defaults to 'euclidean'. Users are encouraged to explore different metrics, such as 'cosine' and 'jaccard'. The 'hamming' and 'jaccard' distances are also available for string vectors. Accepted metrics include NMSLib metrics and sklearn metrics. Some examples are: -'sqeuclidean' -'euclidean' -'l1' -'lp' - requires setting the parameter p -'cosine' -'angular' -'negdotprod' -'levenshtein' -'hamming' -'jaccard' -'jansen-shan' p : int or float (optional, default 11/16 ) P for the Lp metric, when metric='lp' . Can be fractional. The default 11/16 approximates an astroid norm with some computational efficiency (2^n bases are less painstakinly slow to compute). See https://en.wikipedia.org/wiki/Lp_space for some context. transitions : bool (optional, default False) Whether to estimate the diffusion transitions graph. If True , maps a basis encoding neighborhood transitions probability during eigendecomposition. If 'False' (default), maps the diffusion kernel. alpha : int or float (optional, default 1) Alpha in the diffusion maps literature. Controls how much the results are biased by data distribution. Defaults to 1, which is suitable for normalized data. kernel_use : str (optional, default 'decay_adaptive') Which type of kernel to use. There are four implemented, considering the adaptive decay and the neighborhood expansion, written as 'simple', 'decay', 'simple_adaptive' and 'decay_adaptive'. The first, 'simple' , is a locally-adaptive kernel similar to that proposed by Nadler et al.(https://doi.org/10.1016/j.acha.2005.07.004) and implemented in Setty et al. (https://doi.org/10.1038/s41587-019-0068-4). The 'decay' option applies an adaptive decay rate, but no neighborhood expansion. Those, followed by '_adaptive', apply the neighborhood expansion process. The default and recommended is 'decay_adaptive'. The neighborhood expansion can impact runtime, although this is not usually expressive for datasets under 10e6 samples. transitions : bool (optional, default False) Whether to decompose the transition graph when transforming. norm : bool (optional, default True) Whether to normalize the kernel transition probabilities to approximate the LPO. eigengap : bool (optional, default True) Whether to expand the eigendecomposition a bit and stop if eigenvalues sign shift (limit of float64). Used to guarantee numerical stability. n_jobs : int (optional, default 4) Number of threads to use in calculations. Defaults to 4 for safety, but performance scales dramatically when using more threads. plot_spectrum : bool (optional, default False) Whether to plot the spectrum decay analysis. verbose : bool (optional, default False) Controls verbosity. cache : bool (optional, default True) Whether to cache nearest-neighbors (before fit) and to store diffusion matrices after mapping (before transform).","title":"Parameters"},{"location":"diffusor/#topo.tpgraph.diffusion.Diffusor--example","text":"import numpy as np from sklearn.datasets import load_digits from scipy.sparse import csr_matrix from topo.tpgraph.diffusion import Diffusor digits = load_digits() data = csr_matrix(digits) diff = Diffusor().fit(data) msdiffmap = diff.transform(data)","title":"Example"},{"location":"diffusor/#topo.tpgraph.diffusion.Diffusor.fit","text":"Fits an adaptive anisotropic diffusion kernel to the data.","title":"fit()"},{"location":"diffusor/#topo.tpgraph.diffusion.Diffusor.fit--parameters","text":"X : input data. Takes in numpy arrays and scipy csr sparse matrices. Use with sparse data for top performance. You can adjust a series of parameters that can make the process faster and more informational depending on your dataset.","title":"Parameters"},{"location":"diffusor/#topo.tpgraph.diffusion.Diffusor.fit--returns","text":"Diffusor object with kernel Diffusor.K and the transition potencial Diffusor.T . Source code in topo/tpgraph/diffusion.py def fit ( self , X ): \"\"\" Fits an adaptive anisotropic diffusion kernel to the data. Parameters ---------- X : input data. Takes in numpy arrays and scipy csr sparse matrices. Use with sparse data for top performance. You can adjust a series of parameters that can make the process faster and more informational depending on your dataset. Returns ------- Diffusor object with kernel Diffusor.K and the transition potencial Diffusor.T . \"\"\" data = X self . start_time = time . time () self . N = data . shape [ 0 ] self . M = data . shape [ 1 ] if self . kernel_use not in [ 'simple' , 'simple_adaptive' , 'decay' , 'decay_adaptive' ]: raise Exception ( 'Kernel must be either \\' simple \\' , \\' simple_adaptive \\' , \\' decay \\' or \\' decay_adaptive \\' .' ) if self . ann : # Construct an approximate k-nearest-neighbors graph anbrs = ann . NMSlibTransformer ( n_neighbors = self . n_neighbors , metric = self . metric , p = self . p , method = 'hnsw' , n_jobs = self . n_jobs , M = self . M , efC = self . efC , efS = self . efS , verbose = self . verbose ) . fit ( data ) knn = anbrs . transform ( data ) # X, y specific stds: Normalize by the distance of median nearest neighbor to account for neighborhood size. median_k = np . floor ( self . n_neighbors / 2 ) . astype ( np . int ) adap_sd = np . zeros ( self . N ) for i in np . arange ( len ( adap_sd )): adap_sd [ i ] = np . sort ( knn . data [ knn . indptr [ i ]: knn . indptr [ i + 1 ]])[ median_k - 1 ] else : if self . metric == 'lp' : raise Exception ( 'Generalized Lp distances are available only with `ann` set to True.' ) # Construct a k-nearest-neighbors graph nbrs = NearestNeighbors ( n_neighbors = int ( self . n_neighbors ), metric = self . metric , n_jobs = self . n_jobs ) . fit ( data ) knn = nbrs . kneighbors_graph ( data , mode = 'distance' ) # X, y specific stds: Normalize by the distance of median nearest neighbor to account for neighborhood size. median_k = np . floor ( self . n_neighbors / 2 ) . astype ( np . int ) adap_sd = np . zeros ( self . N ) for i in np . arange ( len ( adap_sd )): adap_sd [ i ] = np . sort ( knn . data [ knn . indptr [ i ]: knn . indptr [ i + 1 ]])[ median_k - 1 ] # Distance metrics x , y , dists = find ( knn ) # k-nearest-neighbor distances if self . cache : self . dists = knn self . adap_sd = adap_sd # Neighborhood graph expansion # define decay as sample's pseudomedian k-nearest-neighbor pm = np . interp ( adap_sd , ( adap_sd . min (), adap_sd . max ()), ( 2 , self . n_neighbors )) self . omega = pm # adaptive neighborhood size if self . kernel_use == 'simple_adaptive' or self . kernel_use == 'decay_adaptive' : self . new_k = int ( self . n_neighbors + ( self . n_neighbors - pm . max ())) # increase neighbor search: anbrs_new = ann . NMSlibTransformer ( n_neighbors = self . new_k , metric = self . metric , method = 'hnsw' , n_jobs = self . n_jobs , p = self . p , M = self . M , efC = self . efC , efS = self . efS ) . fit ( data ) knn_new = anbrs_new . transform ( data ) x_new , y_new , dists_new = find ( knn_new ) # adaptive neighborhood size adap_nbr = np . zeros ( self . N ) for i in np . arange ( len ( adap_nbr )): adap_k = int ( np . floor ( pm [ i ])) adap_nbr [ i ] = np . sort ( knn_new . data [ knn_new . indptr [ i ]: knn_new . indptr [ i + 1 ]])[ adap_k - 1 ] if self . cache : self . dists_new = knn_new self . adap_nbr_sd = adap_nbr if self . kernel_use == 'simple' : # X, y specific stds dists = dists / ( adap_sd [ x ] + 1e-10 ) # Normalize by the distance of median nearest neighbor W = csr_matrix (( np . exp ( - dists ), ( x , y )), shape = [ self . N , self . N ]) if self . kernel_use == 'simple_adaptive' : # X, y specific stds dists = dists_new / ( adap_nbr [ x_new ] + 1e-10 ) # Normalize by normalized contribution to neighborhood size. W = csr_matrix (( np . exp ( - dists ), ( x_new , y_new )), shape = [ self . N , self . N ]) if self . kernel_use == 'decay' : # X, y specific stds dists = ( dists / ( adap_sd [ x ] + 1e-10 )) ** np . power ( 2 , (( self . n_neighbors - pm [ x ]) / pm [ x ])) W = csr_matrix (( np . exp ( - dists ), ( x , y )), shape = [ self . N , self . N ]) if self . kernel_use == 'decay_adaptive' : # X, y specific stds dists = ( dists_new / ( adap_nbr [ x_new ] + 1e-10 )) ** np . power ( 2 , ((( int ( self . n_neighbors + ( self . n_neighbors - pm . max ()))) - pm [ x_new ]) / pm [ x_new ])) # Normalize by normalized contribution to neighborhood size. W = csr_matrix (( np . exp ( - dists ), ( x_new , y_new )), shape = [ self . N , self . N ]) # Kernel construction kernel = ( W + W . T ) / 2 self . K = kernel # handle nan, zeros self . K . data = np . where ( np . isnan ( self . K . data ), 1 , self . K . data ) # Diffusion through Markov chain D = np . ravel ( self . K . sum ( axis = 1 )) if self . alpha > 0 : # L_alpha D [ D != 0 ] = D [ D != 0 ] ** ( - self . alpha ) mat = csr_matrix (( D , ( range ( self . N ), range ( self . N ))), shape = [ self . N , self . N ]) kernel = mat . dot ( self . K ) . dot ( mat ) D = np . ravel ( kernel . sum ( axis = 1 )) D [ D != 0 ] = 1 / D [ D != 0 ] # Setting the diffusion operator if self . norm : self . K = kernel self . T = csr_matrix (( D , ( range ( self . N ), range ( self . N ))), shape = [ self . N , self . N ]) . dot ( self . K ) else : self . T = csr_matrix (( D , ( range ( self . N ), range ( self . N ))), shape = [ self . N , self . N ]) . dot ( self . K ) # Guarantee symmetry self . T = ( self . T + self . T . T ) / 2 self . T [( np . arange ( self . T . shape [ 0 ]), np . arange ( self . T . shape [ 0 ]))] = 0 end = time . time () if self . verbose : print ( 'Diffusion time = %f (sec), per sample= %f (sec), per sample adjusted for thread number= %f (sec)' % ( end - self . start_time , float ( end - self . start_time ) / self . N , self . n_jobs * float ( end - self . start_time ) / self . N )) return self","title":"Returns"},{"location":"diffusor/#topo.tpgraph.diffusion.Diffusor.ind_dist_grad","text":"Utility function to get indices, distances and gradients from a multiscale diffusion map.","title":"ind_dist_grad()"},{"location":"diffusor/#topo.tpgraph.diffusion.Diffusor.ind_dist_grad--parameters","text":"data : Input data matrix (numpy array, pandas df, csr_matrix). !!! n_components \"int (optional, default None)\" Numper of components to map to prior to learning.","title":"Parameters"},{"location":"diffusor/#topo.tpgraph.diffusion.Diffusor.ind_dist_grad--returns","text":"A tuple containing neighborhood indices, distances, gradient and a knn graph. Source code in topo/tpgraph/diffusion.py def ind_dist_grad ( self , data ): \"\"\" Utility function to get indices, distances and gradients from a multiscale diffusion map. Parameters ---------- data : Input data matrix (numpy array, pandas df, csr_matrix). n_components: int (optional, default None) Numper of components to map to prior to learning. Returns ------- A tuple containing neighborhood indices, distances, gradient and a knn graph. \"\"\" self . start_time = time . time () # Fit an optimal number of components based on the eigengap # Use user's or default initial guess multiplier = self . N // 10e4 # initial eigen value decomposition if self . transitions : D , V = eigs ( self . T , self . n_components , tol = 1e-4 , maxiter = self . N ) else : D , V = eigs ( self . K , self . n_components , tol = 1e-4 , maxiter = self . N ) D = np . real ( D ) V = np . real ( V ) inds = np . argsort ( D )[:: - 1 ] D = D [ inds ] V = V [:, inds ] # Normalize by the first diffusion component for i in range ( V . shape [ 1 ]): V [:, i ] = V [:, i ] / np . linalg . norm ( V [:, i ]) vals = np . array ( V ) pos = np . sum ( vals > 0 , axis = 0 ) residual = np . sum ( vals < 0 , axis = 0 ) if self . eigengap and len ( residual ) < 1 : #expand eigendecomposition target = self . n_components * multiplier while residual < 3 : print ( 'Eigengap not found for determined number of components. Expanding eigendecomposition to ' + str ( target ) + 'components.' ) if self . transitions : D , V = eigs ( self . T , target , tol = 1e-4 , maxiter = self . N ) else : D , V = eigs ( self . K , target , tol = 1e-4 , maxiter = self . N ) D = np . real ( D ) V = np . real ( V ) inds = np . argsort ( D )[:: - 1 ] D = D [ inds ] V = V [:, inds ] # Normalize by the first diffusion component for i in range ( V . shape [ 1 ]): V [:, i ] = V [:, i ] / np . linalg . norm ( V [:, i ]) vals = np . array ( V ) residual = np . sum ( vals < 0 , axis = 0 ) target = target * 2 if len ( residual ) > 30 : self . n_components = len ( pos ) + 15 # adapted eigen value decomposition if self . transitions : D , V = eigs ( self . T , self . n_components , tol = 1e-4 , maxiter = self . N ) else : D , V = eigs ( self . K , self . n_components , tol = 1e-4 , maxiter = self . N ) D = np . real ( D ) V = np . real ( V ) inds = np . argsort ( D )[:: - 1 ] D = D [ inds ] V = V [:, inds ] # Normalize by the first diffusion component for i in range ( V . shape [ 1 ]): V [:, i ] = V [:, i ] / np . linalg . norm ( V [:, i ]) # Create the results dictionary self . res = { 'EigenVectors' : V , 'EigenValues' : D } self . res [ 'EigenVectors' ] = pd . DataFrame ( self . res [ 'EigenVectors' ]) if not issparse ( data ): self . res [ 'EigenValues' ] = pd . Series ( self . res [ 'EigenValues' ]) self . res [ \"EigenValues\" ] = pd . Series ( self . res [ \"EigenValues\" ]) self . res [ 'MultiscaleComponents' ], self . kn , self . scaled_eigs = multiscale . multiscale ( self . res , n_eigs = self . use_eigs , verbose = self . verbose ) anbrs = ann . NMSlibTransformer ( n_neighbors = self . n_neighbors , metric = 'cosine' , method = 'hnsw' , n_jobs = self . n_jobs , M = self . M , efC = self . efC , efS = self . efS , dense = True , verbose = self . verbose ) . fit ( self . res [ 'MultiscaleComponents' ]) ind , dists , grad , graph = anbrs . ind_dist_grad ( self . res [ 'MultiscaleComponents' ]) end = time . time () print ( 'Diffusion time = %f (sec), per sample= %f (sec), per sample adjusted for thread number= %f (sec)' % ( end - self . start_time , float ( end - self . start_time ) / self . N , self . n_jobs * float ( end - self . start_time ) / self . N )) if self . plot_spectrum : self . spectrum_plot () return ind , dists , grad , graph","title":"Returns"},{"location":"diffusor/#topo.tpgraph.diffusion.Diffusor.rescale","text":"Re-scale the multiscale procedure to a new number of components.","title":"rescale()"},{"location":"diffusor/#topo.tpgraph.diffusion.Diffusor.rescale--parameters","text":"self : Diffusor object. n_eigs : int. Number of diffusion components to multiscale.","title":"Parameters"},{"location":"diffusor/#topo.tpgraph.diffusion.Diffusor.rescale--returns","text":"np.ndarray containing the new multiscaled basis. Source code in topo/tpgraph/diffusion.py def rescale ( self , n_eigs = None ): \"\"\" Re-scale the multiscale procedure to a new number of components. Parameters ---------- self : Diffusor object. n_eigs : int. Number of diffusion components to multiscale. Returns ------- np.ndarray containing the new multiscaled basis. \"\"\" if n_eigs is None : n_eigs = self . n_components mms , self . kn , self . scaled_eigs = multiscale . multiscale ( self . res , n_eigs = n_eigs , verbose = self . verbose ) self . res [ 'MultiscaleComponents' ] = mms return mms","title":"Returns"},{"location":"diffusor/#topo.tpgraph.diffusion.Diffusor.spectrum_plot","text":"Plot the decay spectra.","title":"spectrum_plot()"},{"location":"diffusor/#topo.tpgraph.diffusion.Diffusor.spectrum_plot--parameters","text":"self : Diffusor object. bla : Here only for autodoc's sake.","title":"Parameters"},{"location":"diffusor/#topo.tpgraph.diffusion.Diffusor.spectrum_plot--returns","text":"A nice plot of the diffusion spectra. Source code in topo/tpgraph/diffusion.py def spectrum_plot ( self , bla = None ): \"\"\" Plot the decay spectra. Parameters ---------- self : Diffusor object. bla : Here only for autodoc's sake. Returns ------- A nice plot of the diffusion spectra. \"\"\" if self . kn is None : msc , self . kn , self . scaled_eigs = multiscale . multiscale ( self . res , n_eigs = self . use_eigs , verbose = self . verbose ) ax1 = plt . subplot ( 2 , 1 , 1 ) ax1 . set_title ( 'Spectrum decay and \\' knee \\' ( %i )' % int ( self . kn . knee )) ax1 . plot ( self . kn . x , self . kn . y , 'b' , label = 'data' ) ax1 . set_ylabel ( 'Eigenvalues' ) ax1 . set_xlabel ( 'Eigenvectors' ) ax1 . vlines ( self . kn . knee , plt . ylim ()[ 0 ], plt . ylim ()[ 1 ], linestyles = \"--\" , label = 'Knee' ) ax1 . legend ( loc = 'best' ) ax2 = plt . subplot ( 2 , 1 , 2 ) ax2 . set_title ( 'Curve analysis' ) ax2 . plot ( self . kn . x_normalized , self . kn . y_normalized , \"b\" , label = \"normalized\" ) ax2 . plot ( self . kn . x_difference , self . kn . y_difference , \"r\" , label = \"differential\" ) ax2 . set_xticks ( np . arange ( self . kn . x_normalized . min (), self . kn . x_normalized . max () + 0.1 , 0.1 ) ) ax2 . set_yticks ( np . arange ( self . kn . y_difference . min (), self . kn . y_normalized . max () + 0.1 , 0.1 ) ) ax2 . vlines ( self . kn . norm_knee , plt . ylim ()[ 0 ], plt . ylim ()[ 1 ], linestyles = \"--\" , label = \"Knee\" , ) ax2 . legend ( loc = \"best\" ) plt . tight_layout () plt . show () return plt","title":"Returns"},{"location":"diffusor/#topo.tpgraph.diffusion.Diffusor.transform","text":"Fits the renormalized Laplacian approximating the Laplace Beltrami-Operator in a discrete eigendecomposition. Then multiscales the resulting components. Parameters X : input data. Takes in numpy arrays and scipy csr sparse matrices. Use with sparse data for top performance. You can adjust a series of parameters that can make the process faster and more informational depending on your dataset. Returns Diffusor.res['MultiscaleComponents']] Source code in topo/tpgraph/diffusion.py def transform ( self , X ): \"\"\" Fits the renormalized Laplacian approximating the Laplace Beltrami-Operator in a discrete eigendecomposition. Then multiscales the resulting components. Parameters ---------- X : input data. Takes in numpy arrays and scipy csr sparse matrices. Use with sparse data for top performance. You can adjust a series of parameters that can make the process faster and more informational depending on your dataset. Returns ------- ``Diffusor.res['MultiscaleComponents']]`` \"\"\" self . start_time = time . time () # Fit an optimal number of components based on the eigengap # Use user's or default initial guess # initial eigen value decomposition if self . transitions : D , V = eigs ( self . T , self . n_components , tol = 1e-4 , maxiter = ( self . N // 10 )) else : D , V = eigs ( self . K , self . n_components , tol = 1e-4 , maxiter = ( self . N // 10 )) D = np . real ( D ) V = np . real ( V ) inds = np . argsort ( D )[:: - 1 ] D = D [ inds ] V = V [:, inds ] # Normalize by the first diffusion component for i in range ( V . shape [ 1 ]): V [:, i ] = V [:, i ] / np . linalg . norm ( V [:, i ]) vals = np . array ( V ) pos = np . sum ( vals > 0 , axis = 0 ) residual = np . sum ( vals < 0 , axis = 0 ) if self . eigengap and len ( residual ) < 1 : #expand eigendecomposition target = self . n_components + 30 while residual < 3 : print ( 'Eigengap not found for determined number of components. Expanding eigendecomposition to ' + str ( target ) + 'components.' ) if self . transitions : D , V = eigs ( self . T , target , tol = 1e-4 , maxiter = ( self . N // 10 )) else : D , V = eigs ( self . K , target , tol = 1e-4 , maxiter = ( self . N // 10 )) D = np . real ( D ) V = np . real ( V ) inds = np . argsort ( D )[:: - 1 ] D = D [ inds ] V = V [:, inds ] # Normalize by the first diffusion component for i in range ( V . shape [ 1 ]): V [:, i ] = V [:, i ] / np . linalg . norm ( V [:, i ]) vals = np . array ( V ) residual = np . sum ( vals < 0 , axis = 0 ) pos = np . sum ( vals > 0 , axis = 0 ) target = int ( target * 1.6 ) if len ( residual ) > 30 : self . n_components = len ( pos ) + 5 # adapted eigen value decomposition if self . transitions : D , V = eigs ( self . T , self . n_components , tol = 1e-4 , maxiter = self . N ) else : D , V = eigs ( self . K , self . n_components , tol = 1e-4 , maxiter = self . N ) D = np . real ( D ) V = np . real ( V ) inds = np . argsort ( D )[:: - 1 ] D = D [ inds ] V = V [:, inds ] if not self . cache : del self . K del self . T # Create the results dictionary self . res = { 'EigenVectors' : V , 'EigenValues' : D } self . res [ 'EigenVectors' ] = pd . DataFrame ( self . res [ 'EigenVectors' ]) self . res [ \"EigenValues\" ] = pd . Series ( self . res [ \"EigenValues\" ]) self . res [ 'MultiscaleComponents' ], self . kn , self . scaled_eigs = multiscale . multiscale ( self . res , verbose = self . verbose ) end = time . time () if self . verbose : print ( 'Multiscale decomposition time = %f (sec), per sample= %f (sec), per sample adjusted for thread number= %f (sec)' % ( end - self . start_time , float ( end - self . start_time ) / self . N , self . n_jobs * float ( end - self . start_time ) / self . N )) if self . plot_spectrum : self . spectrum_plot () return self . res [ 'MultiscaleComponents' ]","title":"transform()"},{"location":"installation/","text":"TopOMetry requires some pre-existing libraries to power its scalability and flexibility. TopOMetry is implemented in python and builds complex, high-level models inherited from scikit-learn BaseEstimator , making it flexible and easy to apply and/or combine with different workflows on virtually any domain. scikit-learn - for general algorithms ANNOY - for optimized neighbor index search nmslib - for fast and accurate k-nearest-neighbors kneed - for finding nice cuttofs pyMDE - for optimizing layouts Prior to installing TopOMetry, make sure you have cmake , scikit-build and setuptools available in your system. If using Linux: sudo apt-get install cmake pip3 install scikit-build setuptools We're also going to need NMSlib for really fast approximate nearest-neighborhood search across different distance metrics. If your CPU supports advanced instructions, we recommend you install nmslib separately for better performance: pip3 install --no-binary :all: nmslib Then, you can install TopOMetry and its other requirements with pip: pip3 install numpy pandas annoy scipy numba torch scikit-learn kneed pymde pip3 install topometry Alternatevely, clone this repo and build from source: git clone https://github.com/davisidarta/topometry cd topometry pip3 install .","title":"Installation"},{"location":"quickstart/","text":"From a large data matrix data (np.ndarray, pd.DataFrame or sp.csr_matrix), you can set up a TopoGraph with default parameters: import topo.models as tp # Learn topological metrics and basis from data. The default is to use diffusion harmonics. tg = tp.TopOGraph() tg = tg.fit(data) Note: topo.ml is the high-level model module which contains the TopOGraph object. After learning a topological basis, we can access topological metrics and basis in the TopOGraph object, and build different topological graphs. # Learn a topological graph. Again, the default is to use diffusion harmonics. tgraph = tg.transform(data) Then, it is possible to optimize the topological graph layout. The first option is to do so with our adaptation of UMAP (MAP), which will minimize the cross-entropy between the topological basis and its graph: # Graph layout optimization with MAP map_emb, aux = tp.MAP(tg.MSDiffMaps, tgraph) The second, albeit most interesting option is to use pyMDE to find a Minimum Distortion Embedding. TopOMetry implements some custom MDE problems within the TopOGraph model : # Set up MDE problem mde = tg.MDE(tgraph) mde_emb = mde.embed()","title":"Quick-start"},{"location":"topograph/","text":"Documentation for TopOGraph topo.models.TopOGraph Convenient TopOMetry class for building, clustering and visualizing n-order topological graphs. From data, builds a topologically-oriented basis with optimized diffusion maps or a continuous k-nearest-neighbors Laplacian Eigenmap, and from this basis learns a topological graph (using a new diffusion process or a continuous kNN kernel). This model approximates the Laplace-Beltrami Operator multiple ways by different ways, depending on the user setup. The topological graph can then be visualized in two or three dimensions with Minimum Distortion Embeddings, which also allows for flexible setup and domain-adaptation. Alternatively, users can explore multiple classes for graph layout optimization in topo.layout . Parameters base_knn : int (optional, default 10). Number of k-nearest-neighbors to compute the Diffusor base operator on. The adaptive kernel will normalize distances by each cell distance of its median neighbor. Nonetheless, this hyperparameter remains as an user input regarding the minimal sample neighborhood resolution that drives the computation of the diffusion metrics. For practical purposes, the minimum amount of samples one would expect to constitute a neighborhood of its own. Increasing k can generate more globally-comprehensive metrics and maps, to a certain extend, however at the expense of fine-grained resolution. More generally, consider this a calculus discretization threshold. graph_knn : int (optional, default 10). Number of k-nearest-neighbors to compute the graph operator on. The adaptive kernel will normalize distances by each cell distance of its median neighbor. Nonetheless, this hyperparameter remains as an user input regarding the minimal sample neighborhood resolution that drives the computation of the diffusion metrics. For practical purposes, the minimum amount of samples one would expect to constitute a neighborhood of its own. Increasing k can generate more globally-comprehensive metrics and maps, to a certain extend, however at the expense of fine-grained resolution. More generally, consider this a calculus discretization threshold. n_eigs : int (optional, default 50). Number of components to compute. This number can be iterated to get different views from data at distinct spectral resolutions. If basis is set to diffusion , this is the number of computed diffusion components. If basis is set to continuous , this is the number of computed eigenvectors of the Laplacian Eigenmaps from the continuous affinity matrix. basis : 'diffusion' or 'continuous' (optional, default 'diffusion'). Which topological basis to build from data. If diffusion , performs an optimized, anisotropic, adaptive diffusion mapping (default). If continuous , computes affinities from continuous k-nearest-neighbors, and a topological basis from the Laplacian Eigenmaps of such metric. graph : 'diff' or 'cknn' (optional, default 'diff'). Which topological graph to learn from the built basis. If 'diff', uses a second-order diffusion process to learn similarities and transition probabilities. If 'cknn', uses the continuous k-nearest-neighbors algorithms. Both algorithms learn graph-oriented topological metrics from the learned basis. ann : bool (optional, default True). Whether to use approximate nearest neighbors for graph construction. If False , uses sklearn default implementation. base_metric : str (optional, default 'cosine'). Distance metrics for building a approximate kNN graphs. Defaults to 'cosine'. Users are encouraged to explore different metrics, such as 'cosine' and 'jaccard'. The 'hamming' and 'jaccard' distances are also available for string vectors. Accepted metrics include NMSLib metrics and sklearn metrics. Some examples are: -'sqeuclidean' -'euclidean' -'l1' -'lp' - requires setting the parameter ``p`` -'cosine' -'angular' -'negdotprod' -'levenshtein' -'hamming' -'jaccard' -'jansen-shan' graph_metric : str (optional, default 'cosine'). Exactly the same as base_matric, but used for building the topological graph. p : int or float (optional, default 11/16 ) P for the Lp metric, when metric='lp' . Can be fractional. The default 11/16 approximates an astroid norm with some computational efficiency (2^n bases are less painstakinly slow to compute). transitions : bool (optional, default False) Whether to estimate the diffusion transitions graph. If True , maps a basis encoding neighborhood transitions probability during eigendecomposition. If 'False' (default), maps the diffusion kernel. alpha : int or float (optional, default 1) Alpha in the diffusion maps literature. Controls how much the results are biased by data distribution. Defaults to 1, which is suitable for normalized data. kernel_use : str (optional, default 'decay_adaptive') Which type of kernel to use in the diffusion approach. There are four implemented, considering the adaptive decay and the neighborhood expansion, written as 'simple', 'decay', 'simple_adaptive' and 'decay_adaptive'. The first, 'simple', is a locally-adaptive kernel similar to that proposed by Nadler et al. (https://doi.org/10.1016/j.acha.2005.07.004) and implemented in Setty et al. (https://doi.org/10.1038/s41587-019-0068-4). The 'decay' option applies an adaptive decay rate, but no neighborhood expansion. Those, followed by '_adaptive', apply the neighborhood expansion process. The default and recommended is 'decay_adaptive'. The neighborhood expansion can impact runtime, although this is not usually expressive for datasets under 10e6 samples. transitions : bool (optional, default False). Whether to decompose the transition graph when fitting the diffusion basis. n_jobs : int. Number of threads to use in calculations. Defaults to all but one. verbose : bool (optional, default False). Controls verbosity. cache : bool (optional, default True). Whether to cache nearest-neighbors (before fit) and to store diffusion matrices after mapping (before transform). fit ( self , data ) Learn topological distances with diffusion harmonics and continuous metrics. Computes affinity operators that approximate the Laplace-Beltrami operator Parameters data : High-dimensional data matrix. Currently, supports only data from similar type (i.e. all bool, all float) Returns TopoGraph instance with several slots, populated as per user settings. If basis=diffusion , populates TopoGraph.MSDiffMap with a multiscale diffusion mapping of data, and TopoGraph.DiffBasis with a fitted topo.tpgraph.diff.Diffusor() class containing diffusion metrics and transition probabilities, respectively stored in TopoGraph.DiffBasis.K and TopoGraph.DiffBasis.T If basis=continuous , populates TopoGraph.CLapMap with a continous Laplacian Eigenmapping of data, and TopoGraph.ContBasis with a continuous-k-nearest-neighbors model, containing continuous metrics and adjacency, respectively stored in TopoGraph.ContBasis.K and TopoGraph.ContBasis.A . Source code in topo/models.py def fit ( self , data ): \"\"\" Learn topological distances with diffusion harmonics and continuous metrics. Computes affinity operators that approximate the Laplace-Beltrami operator Parameters ---------- data : High-dimensional data matrix. Currently, supports only data from similar type (i.e. all bool, all float) Returns ------- TopoGraph instance with several slots, populated as per user settings. If `basis=diffusion`, populates `TopoGraph.MSDiffMap` with a multiscale diffusion mapping of data, and `TopoGraph.DiffBasis` with a fitted `topo.tpgraph.diff.Diffusor()` class containing diffusion metrics and transition probabilities, respectively stored in TopoGraph.DiffBasis.K and TopoGraph.DiffBasis.T If `basis=continuous`, populates `TopoGraph.CLapMap` with a continous Laplacian Eigenmapping of data, and `TopoGraph.ContBasis` with a continuous-k-nearest-neighbors model, containing continuous metrics and adjacency, respectively stored in `TopoGraph.ContBasis.K` and `TopoGraph.ContBasis.A`. \"\"\" self . N = data . shape [ 0 ] self . M = data . shape [ 1 ] if self . random_state is None : self . random_state = random . RandomState () print ( 'Building topological basis...' ) if self . basis == 'diffusion' : start = time . time () self . DiffBasis = Diffusor ( n_components = self . n_eigs , n_neighbors = self . base_knn , alpha = self . alpha , n_jobs = self . n_jobs , ann = self . ann , metric = self . base_metric , p = self . p , M = self . M , efC = self . efC , efS = self . efS , kernel_use = self . kernel_use , norm = self . norm , transitions = self . transitions , eigengap = self . eigengap , verbose = self . verbose , plot_spectrum = self . plot_spectrum , cache = self . cache_base ) self . MSDiffMap = self . DiffBasis . fit_transform ( data ) end = time . time () print ( 'Topological basis fitted with diffusion mappings in %f (sec)' % ( end - start )) elif self . basis == 'continuous' : start = time . time () self . ContBasis = cknn_graph ( data , n_neighbors = self . base_knn , delta = self . delta , metric = self . base_metric , t = self . t , include_self = True , is_sparse = True , return_instance = True ) self . CLapMap = spt . LapEigenmap ( self . ContBasis . K , self . n_eigs , self . random_state , ) expansion = 10.0 / np . abs ( self . CLapMap ) . max () self . CLapMap = ( self . CLapMap * expansion ) . astype ( np . float32 ) + self . random_state . normal ( scale = 0.0001 , size = [ self . ContBasis . K . shape [ 0 ], self . n_eigs ] ) . astype ( np . float32 ) end = time . time () print ( 'Topological basis fitted with continuous mappings in %f (sec)' % ( end - start )) return self MAP ( self , data , graph , dims = 2 , min_dist = 0.3 , spread = 1.2 , initial_alpha = 1 , n_epochs = 500 , metric = 'cosine' , metric_kwds = {}, output_metric = 'euclidean' , output_metric_kwds = {}, gamma = 1.2 , negative_sample_rate = 10 , init = 'spectral' , random_state = None , euclidean_output = True , parallel = True , njobs =- 1 , verbose = False , densmap = False , densmap_kwds = {}, output_dens = False ) \"\" Manifold Approximation and Projection, as proposed by Leland McInnes with an uniform distribution assumption in the seminal UMAP algorithm . Perform a fuzzy simplicial set embedding, using a specified initialisation method and then minimizing the fuzzy set cross entropy between the 1-skeletons of the high and low dimensional fuzzy simplicial sets. The fuzzy simplicial set embedding was proposed and implemented by Leland McInnes in UMAP (see umap-learn <https://github.com/lmcinnes/umap> ). Here we're using it only for the projection (layout optimization) by minimizing the cross-entropy between a phenotypic map (i.e. data, TopOMetry latent mappings) and its graph topological representation. Parameters !!! data \"array of shape (n_samples, n_features)\" The source data to be embedded by UMAP. !!! graph \"sparse matrix\" The 1-skeleton of the high dimensional fuzzy simplicial set as represented by a graph for which we require a sparse matrix for the (weighted) adjacency matrix. !!! n_components \"int\" The dimensionality of the euclidean space into which to embed the data. !!! initial_alpha \"float\" Initial learning rate for the SGD. !!! a \"float\" Parameter of differentiable approximation of right adjoint functor !!! b \"float\" Parameter of differentiable approximation of right adjoint functor !!! gamma \"float\" Weight to apply to negative samples. !!! negative_sample_rate \"int (optional, default 5)\" The number of negative samples to select per positive sample in the optimization process. Increasing this value will result in greater repulsive force being applied, greater optimization cost, but slightly more accuracy. !!! n_epochs \"int (optional, default 0)\" The number of training epochs to be used in optimizing the low dimensional embedding. Larger values result in more accurate embeddings. If 0 is specified a value will be selected based on the size of the input dataset (200 for large datasets, 500 for small). !!! init \"string\" How to initialize the low dimensional embedding. Options are: * 'spectral': use a spectral embedding of the fuzzy 1-skeleton * 'random': assign initial embedding positions at random. * A numpy array of initial embedding positions. !!! random_state \"numpy RandomState or equivalent\" A state capable being used as a numpy random state. !!! metric \"string or callable\" The metric used to measure distance in high dimensional space; used if multiple connected components need to be layed out. !!! metric_kwds \"dict\" Key word arguments to be passed to the metric function; used if multiple connected components need to be layed out. !!! densmap \"bool\" Whether to use the density-augmented objective function to optimize the embedding according to the densMAP algorithm. !!! densmap_kwds \"dict\" Key word arguments to be used by the densMAP optimization. !!! output_dens \"bool\" Whether to output local radii in the original data and the embedding. !!! output_metric \"function\" Function returning the distance between two points in embedding space and the gradient of the distance wrt the first argument. !!! output_metric_kwds \"dict\" Key word arguments to be passed to the output_metric function. !!! euclidean_output \"bool\" Whether to use the faster code specialised for euclidean output metrics !!! parallel \"bool (optional, default False)\" Whether to run the computation using numba parallel. Running in parallel is non-deterministic, and is not used if a random seed has been set, to ensure reproducibility. !!! return_init \"bool , (optional, default False)\" Whether to also return the multicomponent spectral initialization. !!! verbose \"bool (optional, default False)\" Whether to report information on the current progress of the algorithm. Returns !!! embedding \"array of shape (n_samples, n_components)\" The optimized of graph into an n_components dimensional euclidean space. !!! aux_data \"dict\" Auxiliary dictionary output returned with the embedding. aux_data['Y_init'] : array of shape (n_samples, n_components) The spectral initialization of graph into an n_components dimensional euclidean space. When densMAP extension is turned on, this dictionary includes local radii in the original data (``aux_data['rad_orig']``) and in the embedding (``aux_data['rad_emb']``). Source code in topo/models.py def MAP ( self , data , graph , dims = 2 , min_dist = 0.3 , spread = 1.2 , initial_alpha = 1 , n_epochs = 500 , metric = 'cosine' , metric_kwds = {}, output_metric = 'euclidean' , output_metric_kwds = {}, gamma = 1.2 , negative_sample_rate = 10 , init = 'spectral' , random_state = None , euclidean_output = True , parallel = True , njobs =- 1 , verbose = False , densmap = False , densmap_kwds = {}, output_dens = False , ): \"\"\"\"\" Manifold Approximation and Projection, as proposed by Leland McInnes with an uniform distribution assumption in the seminal [UMAP algorithm](https://umap-learn.readthedocs.io/en/latest/index.html). Perform a fuzzy simplicial set embedding, using a specified initialisation method and then minimizing the fuzzy set cross entropy between the 1-skeletons of the high and low dimensional fuzzy simplicial sets. The fuzzy simplicial set embedding was proposed and implemented by Leland McInnes in UMAP (see `umap-learn <https://github.com/lmcinnes/umap>`). Here we're using it only for the projection (layout optimization) by minimizing the cross-entropy between a phenotypic map (i.e. data, TopOMetry latent mappings) and its graph topological representation. Parameters ---------- data: array of shape (n_samples, n_features) The source data to be embedded by UMAP. graph: sparse matrix The 1-skeleton of the high dimensional fuzzy simplicial set as represented by a graph for which we require a sparse matrix for the (weighted) adjacency matrix. n_components: int The dimensionality of the euclidean space into which to embed the data. initial_alpha: float Initial learning rate for the SGD. a: float Parameter of differentiable approximation of right adjoint functor b: float Parameter of differentiable approximation of right adjoint functor gamma: float Weight to apply to negative samples. negative_sample_rate: int (optional, default 5) The number of negative samples to select per positive sample in the optimization process. Increasing this value will result in greater repulsive force being applied, greater optimization cost, but slightly more accuracy. n_epochs: int (optional, default 0) The number of training epochs to be used in optimizing the low dimensional embedding. Larger values result in more accurate embeddings. If 0 is specified a value will be selected based on the size of the input dataset (200 for large datasets, 500 for small). init: string How to initialize the low dimensional embedding. Options are: * 'spectral': use a spectral embedding of the fuzzy 1-skeleton * 'random': assign initial embedding positions at random. * A numpy array of initial embedding positions. random_state: numpy RandomState or equivalent A state capable being used as a numpy random state. metric: string or callable The metric used to measure distance in high dimensional space; used if multiple connected components need to be layed out. metric_kwds: dict Key word arguments to be passed to the metric function; used if multiple connected components need to be layed out. densmap: bool Whether to use the density-augmented objective function to optimize the embedding according to the densMAP algorithm. densmap_kwds: dict Key word arguments to be used by the densMAP optimization. output_dens: bool Whether to output local radii in the original data and the embedding. output_metric: function Function returning the distance between two points in embedding space and the gradient of the distance wrt the first argument. output_metric_kwds: dict Key word arguments to be passed to the output_metric function. euclidean_output: bool Whether to use the faster code specialised for euclidean output metrics parallel: bool (optional, default False) Whether to run the computation using numba parallel. Running in parallel is non-deterministic, and is not used if a random seed has been set, to ensure reproducibility. return_init: bool , (optional, default False) Whether to also return the multicomponent spectral initialization. verbose: bool (optional, default False) Whether to report information on the current progress of the algorithm. Returns ------- embedding: array of shape (n_samples, n_components) The optimized of ``graph`` into an ``n_components`` dimensional euclidean space. aux_data: dict Auxiliary dictionary output returned with the embedding. ``aux_data['Y_init']``: array of shape (n_samples, n_components) The spectral initialization of ``graph`` into an ``n_components`` dimensional euclidean space. When densMAP extension is turned on, this dictionary includes local radii in the original data (``aux_data['rad_orig']``) and in the embedding (``aux_data['rad_emb']``). \"\"\" \"\" start = time . time () results = uni . fuzzy_embedding ( data , graph , n_components = dims , initial_alpha = initial_alpha , min_dist = min_dist , spread = spread , n_epochs = n_epochs , metric = metric , metric_kwds = metric_kwds , output_metric = output_metric , output_metric_kwds = output_metric_kwds , gamma = gamma , negative_sample_rate = negative_sample_rate , init = init , random_state = random_state , euclidean_output = euclidean_output , parallel = parallel , njobs = njobs , verbose = verbose , a = None , b = None , densmap = densmap , densmap_kwds = densmap_kwds , output_dens = output_dens ) end = time . time () print ( 'Fuzzy layout optimization embedding in = %f (sec)' % ( end - start )) return results MDE ( self , target , data = None , dim = 2 , n_neighbors = None , type = 'isomorphic' , constraint = 'standardized' , init = 'quadratic' , attractive_penalty =< class ' pymde . functions . penalties . Log1p '>, repulsive_penalty=<class ' pymde . functions . penalties . Log '>, loss=<class ' pymde . functions . losses . Absolute '>, repulsive_fraction=None, max_distance=None, device=' cpu ', verbose=False) This function constructs an MDE problem for preserving the structure of original data. This MDE problem is well-suited for visualization (using dim 2 or 3), but can also be used to generate features for machine learning tasks (with dim = 10, 50, or 100, for example). It yields embeddings in which similar items are near each other, and dissimilar items are not near each other. The original data can either be a data matrix, or a graph. Data matrices should be torch Tensors, NumPy arrays, or scipy sparse matrices; graphs should be instances of pymde.Graph . The MDE problem uses distortion functions derived from weights (i.e., penalties). To obtain an embedding, call the embed method on the returned MDE object. To plot it, use pymde.plot . Parameters data : torch.Tensor, numpy.ndarray, scipy.sparse matrix or pymde.Graph. The original data, a data matrix of shape (n_items, n_features) or a graph. Neighbors are computed using Euclidean distance if the data is a matrix, or the shortest-path metric if the data is a graph. dim : int. The embedding dimension. Use 2 or 3 for visualization. attractive_penalty : pymde.Function class (or factory). Callable that constructs a distortion function, given positive weights. Typically one of the classes from pymde.penalties , such as pymde.penalties.log1p , pymde.penalties.Huber , or pymde.penalties.Quadratic . repulsive_penalty : pymde.Function class (or factory). Callable that constructs a distortion function, given negative weights. (If None , only positive weights are used.) For example, pymde.penalties.Log or pymde.penalties.InversePower . constraint : str (optional), default 'standardized'. Constraint to use when optimizing the embedding. Options are 'standardized', 'centered', None or a pymde.constraints.Constraint() function. n_neighbors : int (optional) The number of nearest neighbors to compute for each row (item) of data . A sensible value is chosen by default, depending on the number of items. repulsive_fraction : float (optional) How many repulsive edges to include, relative to the number of attractive edges. 1 means as many repulsive edges as attractive edges. The higher this number, the more uniformly spread out the embedding will be. Defaults to 0.5 for standardized embeddings, and 1 otherwise. (If repulsive_penalty is None , this argument is ignored.) max_distance : float (optional) If not None, neighborhoods are restricted to have a radius no greater than max_distance . init : str or np.ndarray (optional, default 'quadratic') Initialization strategy; np.ndarray, 'quadratic' or 'random'. device : str (optional) Device for the embedding (eg, 'cpu', 'cuda'). verbose : bool If True , print verbose output. Returns torch.tensor A pymde.MDE object, based on the original data. Source code in topo/models.py def MDE ( self , target , data = None , dim = 2 , n_neighbors = None , type = 'isomorphic' , constraint = 'standardized' , init = 'quadratic' , attractive_penalty = penalties . Log1p , repulsive_penalty = penalties . Log , loss = losses . Absolute , repulsive_fraction = None , max_distance = None , device = 'cpu' , verbose = False ): \"\"\" This function constructs an MDE problem for preserving the structure of original data. This MDE problem is well-suited for visualization (using ``dim`` 2 or 3), but can also be used to generate features for machine learning tasks (with ``dim`` = 10, 50, or 100, for example). It yields embeddings in which similar items are near each other, and dissimilar items are not near each other. The original data can either be a data matrix, or a graph. Data matrices should be torch Tensors, NumPy arrays, or scipy sparse matrices; graphs should be instances of ``pymde.Graph``. The MDE problem uses distortion functions derived from weights (i.e., penalties). To obtain an embedding, call the ``embed`` method on the returned ``MDE`` object. To plot it, use ``pymde.plot``. Parameters ---------- data : torch.Tensor, numpy.ndarray, scipy.sparse matrix or pymde.Graph. The original data, a data matrix of shape ``(n_items, n_features)`` or a graph. Neighbors are computed using Euclidean distance if the data is a matrix, or the shortest-path metric if the data is a graph. dim : int. The embedding dimension. Use 2 or 3 for visualization. attractive_penalty : pymde.Function class (or factory). Callable that constructs a distortion function, given positive weights. Typically one of the classes from ``pymde.penalties``, such as ``pymde.penalties.log1p``, ``pymde.penalties.Huber``, or ``pymde.penalties.Quadratic``. repulsive_penalty : pymde.Function class (or factory). Callable that constructs a distortion function, given negative weights. (If ``None``, only positive weights are used.) For example, ``pymde.penalties.Log`` or ``pymde.penalties.InversePower``. constraint : str (optional), default 'standardized'. Constraint to use when optimizing the embedding. Options are 'standardized', 'centered', `None` or a `pymde.constraints.Constraint()` function. n_neighbors : int (optional) The number of nearest neighbors to compute for each row (item) of ``data``. A sensible value is chosen by default, depending on the number of items. repulsive_fraction : float (optional) How many repulsive edges to include, relative to the number of attractive edges. ``1`` means as many repulsive edges as attractive edges. The higher this number, the more uniformly spread out the embedding will be. Defaults to ``0.5`` for standardized embeddings, and ``1`` otherwise. (If ``repulsive_penalty`` is ``None``, this argument is ignored.) max_distance : float (optional) If not None, neighborhoods are restricted to have a radius no greater than ``max_distance``. init : str or np.ndarray (optional, default 'quadratic') Initialization strategy; np.ndarray, 'quadratic' or 'random'. device : str (optional) Device for the embedding (eg, 'cpu', 'cuda'). verbose : bool If ``True``, print verbose output. Returns ------- torch.tensor A ``pymde.MDE`` object, based on the original data. \"\"\" graph = Graph ( target ) if init == 'spectral' : if data is None : print ( 'Spectral initialization requires input data as argument. Falling back to quadratic...' ) init = 'quadratic' else : init = self . spectral_layout ( data , dim ) if constraint == 'standardized' : constraint_use = constraints . Standardized () elif constraint == 'centered' : constraint_use = constraints . Centered () elif isinstance ( constraint , constraints . Constraint ()): constraint_use = constraint else : constraint_use = None if type == 'isomorphic' : emb = mde . IsomorphicMDE ( graph , attractive_penalty = attractive_penalty , repulsive_penalty = repulsive_penalty , embedding_dim = dim , constraint = constraint_use , n_neighbors = n_neighbors , repulsive_fraction = repulsive_fraction , max_distance = max_distance , init = init , device = device , verbose = verbose ) elif type == 'isometric' : if max_distance is None : max_distance = 5e7 emb = mde . IsometricMDE ( graph , embedding_dim = dim , loss = loss , constraint = constraint_use , max_distances = max_distance , device = device , verbose = verbose ) return np . array ( emb ) spectral_layout ( self , data , target , dim = 2 ) Performs a multicomponent spectral layout of the data and the target similarity matrix. Parameters data : input data target : scipy.sparse.csr.csr_matrix. target similarity matrix. dim : int (optional, default 2) number of dimensions to embed into. Returns np.ndarray containing the resulting embedding. Source code in topo/models.py def spectral_layout ( self , data , target , dim = 2 ): \"\"\" Performs a multicomponent spectral layout of the data and the target similarity matrix. Parameters ---------- data : input data target : scipy.sparse.csr.csr_matrix. target similarity matrix. dim : int (optional, default 2) number of dimensions to embed into. Returns ------- np.ndarray containing the resulting embedding. \"\"\" if self . basis == 'diffusion' : spt_layout = spt . spectral_layout ( data , self . DiffBasis . T , dim , self . random_state , metric = \"precomputed\" , ) expansion = 10.0 / np . abs ( spt_layout ) . max () spt_layout = ( spt_layout * expansion ) . astype ( np . float32 ) + self . random_state . normal ( scale = 0.0001 , size = [ self . DiffBasis . T . shape [ 0 ], dim ] ) . astype ( np . float32 ) elif self . basis == 'continuous' : spt_layout = spt . LapEigenmap ( self . ContBasis . K , dim , self . random_state , metric = \"precomputed\" , ) expansion = 10.0 / np . abs ( spt_layout ) . max () spt_layout = ( spt_layout * expansion ) . astype ( np . float32 ) + self . random_state . normal ( scale = 0.0001 , size = [ self . ContBasis . K . shape [ 0 ], dim ] ) . astype ( np . float32 ) return spt_layout transform ( self , base ) Learns new affinity, topological operators from chosen basis. Parameters self : TopOGraph instance. base : str, optional. Base to use when building the topological graph. Defaults to the active base ( TopOGraph.basis ) Returns scipy.sparse.csr.csr_matrix, containing the similarity matrix that encodes the topological graph. Source code in topo/models.py def transform ( self , base ): \"\"\" Learns new affinity, topological operators from chosen basis. Parameters ---------- self : TopOGraph instance. base : str, optional. Base to use when building the topological graph. Defaults to the active base ( `TopOGraph.basis`) Returns ------- scipy.sparse.csr.csr_matrix, containing the similarity matrix that encodes the topological graph. \"\"\" if base is not None : self . basis = base print ( 'Building topological graph...' ) start = time . time () if self . basis == 'continuous' : use_basis = self . CLapMap elif self . basis == 'diffusion' : use_basis = self . MSDiffMap if self . graph == 'diff' : DiffGraph = Diffusor ( n_neighbors = self . graph_knn , alpha = self . alpha , n_jobs = self . n_jobs , ann = self . ann , metric = self . graph_metric , p = self . p , M = self . M , efC = self . efC , efS = self . efS , kernel_use = 'simple' , norm = self . norm , transitions = self . transitions , eigengap = self . eigengap , verbose = self . verbose , plot_spectrum = self . plot_spectrum , cache = False ) . fit ( use_basis ) if self . cache_graph : self . DiffGraph = DiffGraph . T if self . graph == 'cknn' : CknnGraph = cknn_graph ( use_basis , n_neighbors = self . graph_knn , delta = self . delta , metric = self . graph_metric , t = self . t , include_self = True , is_sparse = True ) if self . cache_graph : self . CknnGraph = CknnGraph end = time . time () print ( 'Topological graph extracted in = %f (sec)' % ( end - start )) if self . graph == 'diff' : return DiffGraph . T elif self . graph == 'cknn' : return CknnGraph else : return self","title":"The TopOGraph model"},{"location":"topograph/#documentation-for-topograph","text":"","title":"Documentation for TopOGraph"},{"location":"topograph/#topo.models.TopOGraph","text":"Convenient TopOMetry class for building, clustering and visualizing n-order topological graphs. From data, builds a topologically-oriented basis with optimized diffusion maps or a continuous k-nearest-neighbors Laplacian Eigenmap, and from this basis learns a topological graph (using a new diffusion process or a continuous kNN kernel). This model approximates the Laplace-Beltrami Operator multiple ways by different ways, depending on the user setup. The topological graph can then be visualized in two or three dimensions with Minimum Distortion Embeddings, which also allows for flexible setup and domain-adaptation. Alternatively, users can explore multiple classes for graph layout optimization in topo.layout . Parameters base_knn : int (optional, default 10). Number of k-nearest-neighbors to compute the Diffusor base operator on. The adaptive kernel will normalize distances by each cell distance of its median neighbor. Nonetheless, this hyperparameter remains as an user input regarding the minimal sample neighborhood resolution that drives the computation of the diffusion metrics. For practical purposes, the minimum amount of samples one would expect to constitute a neighborhood of its own. Increasing k can generate more globally-comprehensive metrics and maps, to a certain extend, however at the expense of fine-grained resolution. More generally, consider this a calculus discretization threshold. graph_knn : int (optional, default 10). Number of k-nearest-neighbors to compute the graph operator on. The adaptive kernel will normalize distances by each cell distance of its median neighbor. Nonetheless, this hyperparameter remains as an user input regarding the minimal sample neighborhood resolution that drives the computation of the diffusion metrics. For practical purposes, the minimum amount of samples one would expect to constitute a neighborhood of its own. Increasing k can generate more globally-comprehensive metrics and maps, to a certain extend, however at the expense of fine-grained resolution. More generally, consider this a calculus discretization threshold. n_eigs : int (optional, default 50). Number of components to compute. This number can be iterated to get different views from data at distinct spectral resolutions. If basis is set to diffusion , this is the number of computed diffusion components. If basis is set to continuous , this is the number of computed eigenvectors of the Laplacian Eigenmaps from the continuous affinity matrix. basis : 'diffusion' or 'continuous' (optional, default 'diffusion'). Which topological basis to build from data. If diffusion , performs an optimized, anisotropic, adaptive diffusion mapping (default). If continuous , computes affinities from continuous k-nearest-neighbors, and a topological basis from the Laplacian Eigenmaps of such metric. graph : 'diff' or 'cknn' (optional, default 'diff'). Which topological graph to learn from the built basis. If 'diff', uses a second-order diffusion process to learn similarities and transition probabilities. If 'cknn', uses the continuous k-nearest-neighbors algorithms. Both algorithms learn graph-oriented topological metrics from the learned basis. ann : bool (optional, default True). Whether to use approximate nearest neighbors for graph construction. If False , uses sklearn default implementation. base_metric : str (optional, default 'cosine'). Distance metrics for building a approximate kNN graphs. Defaults to 'cosine'. Users are encouraged to explore different metrics, such as 'cosine' and 'jaccard'. The 'hamming' and 'jaccard' distances are also available for string vectors. Accepted metrics include NMSLib metrics and sklearn metrics. Some examples are: -'sqeuclidean' -'euclidean' -'l1' -'lp' - requires setting the parameter ``p`` -'cosine' -'angular' -'negdotprod' -'levenshtein' -'hamming' -'jaccard' -'jansen-shan' graph_metric : str (optional, default 'cosine'). Exactly the same as base_matric, but used for building the topological graph. p : int or float (optional, default 11/16 ) P for the Lp metric, when metric='lp' . Can be fractional. The default 11/16 approximates an astroid norm with some computational efficiency (2^n bases are less painstakinly slow to compute). transitions : bool (optional, default False) Whether to estimate the diffusion transitions graph. If True , maps a basis encoding neighborhood transitions probability during eigendecomposition. If 'False' (default), maps the diffusion kernel. alpha : int or float (optional, default 1) Alpha in the diffusion maps literature. Controls how much the results are biased by data distribution. Defaults to 1, which is suitable for normalized data. kernel_use : str (optional, default 'decay_adaptive') Which type of kernel to use in the diffusion approach. There are four implemented, considering the adaptive decay and the neighborhood expansion, written as 'simple', 'decay', 'simple_adaptive' and 'decay_adaptive'. The first, 'simple', is a locally-adaptive kernel similar to that proposed by Nadler et al. (https://doi.org/10.1016/j.acha.2005.07.004) and implemented in Setty et al. (https://doi.org/10.1038/s41587-019-0068-4). The 'decay' option applies an adaptive decay rate, but no neighborhood expansion. Those, followed by '_adaptive', apply the neighborhood expansion process. The default and recommended is 'decay_adaptive'. The neighborhood expansion can impact runtime, although this is not usually expressive for datasets under 10e6 samples. transitions : bool (optional, default False). Whether to decompose the transition graph when fitting the diffusion basis. n_jobs : int. Number of threads to use in calculations. Defaults to all but one. verbose : bool (optional, default False). Controls verbosity. cache : bool (optional, default True). Whether to cache nearest-neighbors (before fit) and to store diffusion matrices after mapping (before transform).","title":"TopOGraph"},{"location":"topograph/#topo.models.TopOGraph.fit","text":"Learn topological distances with diffusion harmonics and continuous metrics. Computes affinity operators that approximate the Laplace-Beltrami operator","title":"fit()"},{"location":"topograph/#topo.models.TopOGraph.fit--parameters","text":"data : High-dimensional data matrix. Currently, supports only data from similar type (i.e. all bool, all float)","title":"Parameters"},{"location":"topograph/#topo.models.TopOGraph.fit--returns","text":"TopoGraph instance with several slots, populated as per user settings. If basis=diffusion , populates TopoGraph.MSDiffMap with a multiscale diffusion mapping of data, and TopoGraph.DiffBasis with a fitted topo.tpgraph.diff.Diffusor() class containing diffusion metrics and transition probabilities, respectively stored in TopoGraph.DiffBasis.K and TopoGraph.DiffBasis.T If basis=continuous , populates TopoGraph.CLapMap with a continous Laplacian Eigenmapping of data, and TopoGraph.ContBasis with a continuous-k-nearest-neighbors model, containing continuous metrics and adjacency, respectively stored in TopoGraph.ContBasis.K and TopoGraph.ContBasis.A . Source code in topo/models.py def fit ( self , data ): \"\"\" Learn topological distances with diffusion harmonics and continuous metrics. Computes affinity operators that approximate the Laplace-Beltrami operator Parameters ---------- data : High-dimensional data matrix. Currently, supports only data from similar type (i.e. all bool, all float) Returns ------- TopoGraph instance with several slots, populated as per user settings. If `basis=diffusion`, populates `TopoGraph.MSDiffMap` with a multiscale diffusion mapping of data, and `TopoGraph.DiffBasis` with a fitted `topo.tpgraph.diff.Diffusor()` class containing diffusion metrics and transition probabilities, respectively stored in TopoGraph.DiffBasis.K and TopoGraph.DiffBasis.T If `basis=continuous`, populates `TopoGraph.CLapMap` with a continous Laplacian Eigenmapping of data, and `TopoGraph.ContBasis` with a continuous-k-nearest-neighbors model, containing continuous metrics and adjacency, respectively stored in `TopoGraph.ContBasis.K` and `TopoGraph.ContBasis.A`. \"\"\" self . N = data . shape [ 0 ] self . M = data . shape [ 1 ] if self . random_state is None : self . random_state = random . RandomState () print ( 'Building topological basis...' ) if self . basis == 'diffusion' : start = time . time () self . DiffBasis = Diffusor ( n_components = self . n_eigs , n_neighbors = self . base_knn , alpha = self . alpha , n_jobs = self . n_jobs , ann = self . ann , metric = self . base_metric , p = self . p , M = self . M , efC = self . efC , efS = self . efS , kernel_use = self . kernel_use , norm = self . norm , transitions = self . transitions , eigengap = self . eigengap , verbose = self . verbose , plot_spectrum = self . plot_spectrum , cache = self . cache_base ) self . MSDiffMap = self . DiffBasis . fit_transform ( data ) end = time . time () print ( 'Topological basis fitted with diffusion mappings in %f (sec)' % ( end - start )) elif self . basis == 'continuous' : start = time . time () self . ContBasis = cknn_graph ( data , n_neighbors = self . base_knn , delta = self . delta , metric = self . base_metric , t = self . t , include_self = True , is_sparse = True , return_instance = True ) self . CLapMap = spt . LapEigenmap ( self . ContBasis . K , self . n_eigs , self . random_state , ) expansion = 10.0 / np . abs ( self . CLapMap ) . max () self . CLapMap = ( self . CLapMap * expansion ) . astype ( np . float32 ) + self . random_state . normal ( scale = 0.0001 , size = [ self . ContBasis . K . shape [ 0 ], self . n_eigs ] ) . astype ( np . float32 ) end = time . time () print ( 'Topological basis fitted with continuous mappings in %f (sec)' % ( end - start )) return self","title":"Returns"},{"location":"topograph/#topo.models.TopOGraph.MAP","text":"\"\" Manifold Approximation and Projection, as proposed by Leland McInnes with an uniform distribution assumption in the seminal UMAP algorithm . Perform a fuzzy simplicial set embedding, using a specified initialisation method and then minimizing the fuzzy set cross entropy between the 1-skeletons of the high and low dimensional fuzzy simplicial sets. The fuzzy simplicial set embedding was proposed and implemented by Leland McInnes in UMAP (see umap-learn <https://github.com/lmcinnes/umap> ). Here we're using it only for the projection (layout optimization) by minimizing the cross-entropy between a phenotypic map (i.e. data, TopOMetry latent mappings) and its graph topological representation.","title":"MAP()"},{"location":"topograph/#topo.models.TopOGraph.MAP--parameters","text":"!!! data \"array of shape (n_samples, n_features)\" The source data to be embedded by UMAP. !!! graph \"sparse matrix\" The 1-skeleton of the high dimensional fuzzy simplicial set as represented by a graph for which we require a sparse matrix for the (weighted) adjacency matrix. !!! n_components \"int\" The dimensionality of the euclidean space into which to embed the data. !!! initial_alpha \"float\" Initial learning rate for the SGD. !!! a \"float\" Parameter of differentiable approximation of right adjoint functor !!! b \"float\" Parameter of differentiable approximation of right adjoint functor !!! gamma \"float\" Weight to apply to negative samples. !!! negative_sample_rate \"int (optional, default 5)\" The number of negative samples to select per positive sample in the optimization process. Increasing this value will result in greater repulsive force being applied, greater optimization cost, but slightly more accuracy. !!! n_epochs \"int (optional, default 0)\" The number of training epochs to be used in optimizing the low dimensional embedding. Larger values result in more accurate embeddings. If 0 is specified a value will be selected based on the size of the input dataset (200 for large datasets, 500 for small). !!! init \"string\" How to initialize the low dimensional embedding. Options are: * 'spectral': use a spectral embedding of the fuzzy 1-skeleton * 'random': assign initial embedding positions at random. * A numpy array of initial embedding positions. !!! random_state \"numpy RandomState or equivalent\" A state capable being used as a numpy random state. !!! metric \"string or callable\" The metric used to measure distance in high dimensional space; used if multiple connected components need to be layed out. !!! metric_kwds \"dict\" Key word arguments to be passed to the metric function; used if multiple connected components need to be layed out. !!! densmap \"bool\" Whether to use the density-augmented objective function to optimize the embedding according to the densMAP algorithm. !!! densmap_kwds \"dict\" Key word arguments to be used by the densMAP optimization. !!! output_dens \"bool\" Whether to output local radii in the original data and the embedding. !!! output_metric \"function\" Function returning the distance between two points in embedding space and the gradient of the distance wrt the first argument. !!! output_metric_kwds \"dict\" Key word arguments to be passed to the output_metric function. !!! euclidean_output \"bool\" Whether to use the faster code specialised for euclidean output metrics !!! parallel \"bool (optional, default False)\" Whether to run the computation using numba parallel. Running in parallel is non-deterministic, and is not used if a random seed has been set, to ensure reproducibility. !!! return_init \"bool , (optional, default False)\" Whether to also return the multicomponent spectral initialization. !!! verbose \"bool (optional, default False)\" Whether to report information on the current progress of the algorithm. Returns !!! embedding \"array of shape (n_samples, n_components)\" The optimized of graph into an n_components dimensional euclidean space. !!! aux_data \"dict\" Auxiliary dictionary output returned with the embedding. aux_data['Y_init'] : array of shape (n_samples, n_components) The spectral initialization of graph into an n_components dimensional euclidean space. When densMAP extension is turned on, this dictionary includes local radii in the original data (``aux_data['rad_orig']``) and in the embedding (``aux_data['rad_emb']``). Source code in topo/models.py def MAP ( self , data , graph , dims = 2 , min_dist = 0.3 , spread = 1.2 , initial_alpha = 1 , n_epochs = 500 , metric = 'cosine' , metric_kwds = {}, output_metric = 'euclidean' , output_metric_kwds = {}, gamma = 1.2 , negative_sample_rate = 10 , init = 'spectral' , random_state = None , euclidean_output = True , parallel = True , njobs =- 1 , verbose = False , densmap = False , densmap_kwds = {}, output_dens = False , ): \"\"\"\"\" Manifold Approximation and Projection, as proposed by Leland McInnes with an uniform distribution assumption in the seminal [UMAP algorithm](https://umap-learn.readthedocs.io/en/latest/index.html). Perform a fuzzy simplicial set embedding, using a specified initialisation method and then minimizing the fuzzy set cross entropy between the 1-skeletons of the high and low dimensional fuzzy simplicial sets. The fuzzy simplicial set embedding was proposed and implemented by Leland McInnes in UMAP (see `umap-learn <https://github.com/lmcinnes/umap>`). Here we're using it only for the projection (layout optimization) by minimizing the cross-entropy between a phenotypic map (i.e. data, TopOMetry latent mappings) and its graph topological representation. Parameters ---------- data: array of shape (n_samples, n_features) The source data to be embedded by UMAP. graph: sparse matrix The 1-skeleton of the high dimensional fuzzy simplicial set as represented by a graph for which we require a sparse matrix for the (weighted) adjacency matrix. n_components: int The dimensionality of the euclidean space into which to embed the data. initial_alpha: float Initial learning rate for the SGD. a: float Parameter of differentiable approximation of right adjoint functor b: float Parameter of differentiable approximation of right adjoint functor gamma: float Weight to apply to negative samples. negative_sample_rate: int (optional, default 5) The number of negative samples to select per positive sample in the optimization process. Increasing this value will result in greater repulsive force being applied, greater optimization cost, but slightly more accuracy. n_epochs: int (optional, default 0) The number of training epochs to be used in optimizing the low dimensional embedding. Larger values result in more accurate embeddings. If 0 is specified a value will be selected based on the size of the input dataset (200 for large datasets, 500 for small). init: string How to initialize the low dimensional embedding. Options are: * 'spectral': use a spectral embedding of the fuzzy 1-skeleton * 'random': assign initial embedding positions at random. * A numpy array of initial embedding positions. random_state: numpy RandomState or equivalent A state capable being used as a numpy random state. metric: string or callable The metric used to measure distance in high dimensional space; used if multiple connected components need to be layed out. metric_kwds: dict Key word arguments to be passed to the metric function; used if multiple connected components need to be layed out. densmap: bool Whether to use the density-augmented objective function to optimize the embedding according to the densMAP algorithm. densmap_kwds: dict Key word arguments to be used by the densMAP optimization. output_dens: bool Whether to output local radii in the original data and the embedding. output_metric: function Function returning the distance between two points in embedding space and the gradient of the distance wrt the first argument. output_metric_kwds: dict Key word arguments to be passed to the output_metric function. euclidean_output: bool Whether to use the faster code specialised for euclidean output metrics parallel: bool (optional, default False) Whether to run the computation using numba parallel. Running in parallel is non-deterministic, and is not used if a random seed has been set, to ensure reproducibility. return_init: bool , (optional, default False) Whether to also return the multicomponent spectral initialization. verbose: bool (optional, default False) Whether to report information on the current progress of the algorithm. Returns ------- embedding: array of shape (n_samples, n_components) The optimized of ``graph`` into an ``n_components`` dimensional euclidean space. aux_data: dict Auxiliary dictionary output returned with the embedding. ``aux_data['Y_init']``: array of shape (n_samples, n_components) The spectral initialization of ``graph`` into an ``n_components`` dimensional euclidean space. When densMAP extension is turned on, this dictionary includes local radii in the original data (``aux_data['rad_orig']``) and in the embedding (``aux_data['rad_emb']``). \"\"\" \"\" start = time . time () results = uni . fuzzy_embedding ( data , graph , n_components = dims , initial_alpha = initial_alpha , min_dist = min_dist , spread = spread , n_epochs = n_epochs , metric = metric , metric_kwds = metric_kwds , output_metric = output_metric , output_metric_kwds = output_metric_kwds , gamma = gamma , negative_sample_rate = negative_sample_rate , init = init , random_state = random_state , euclidean_output = euclidean_output , parallel = parallel , njobs = njobs , verbose = verbose , a = None , b = None , densmap = densmap , densmap_kwds = densmap_kwds , output_dens = output_dens ) end = time . time () print ( 'Fuzzy layout optimization embedding in = %f (sec)' % ( end - start )) return results","title":"Parameters"},{"location":"topograph/#topo.models.TopOGraph.MDE","text":"This function constructs an MDE problem for preserving the structure of original data. This MDE problem is well-suited for visualization (using dim 2 or 3), but can also be used to generate features for machine learning tasks (with dim = 10, 50, or 100, for example). It yields embeddings in which similar items are near each other, and dissimilar items are not near each other. The original data can either be a data matrix, or a graph. Data matrices should be torch Tensors, NumPy arrays, or scipy sparse matrices; graphs should be instances of pymde.Graph . The MDE problem uses distortion functions derived from weights (i.e., penalties). To obtain an embedding, call the embed method on the returned MDE object. To plot it, use pymde.plot .","title":"MDE()"},{"location":"topograph/#topo.models.TopOGraph.MDE--parameters","text":"data : torch.Tensor, numpy.ndarray, scipy.sparse matrix or pymde.Graph. The original data, a data matrix of shape (n_items, n_features) or a graph. Neighbors are computed using Euclidean distance if the data is a matrix, or the shortest-path metric if the data is a graph. dim : int. The embedding dimension. Use 2 or 3 for visualization. attractive_penalty : pymde.Function class (or factory). Callable that constructs a distortion function, given positive weights. Typically one of the classes from pymde.penalties , such as pymde.penalties.log1p , pymde.penalties.Huber , or pymde.penalties.Quadratic . repulsive_penalty : pymde.Function class (or factory). Callable that constructs a distortion function, given negative weights. (If None , only positive weights are used.) For example, pymde.penalties.Log or pymde.penalties.InversePower . constraint : str (optional), default 'standardized'. Constraint to use when optimizing the embedding. Options are 'standardized', 'centered', None or a pymde.constraints.Constraint() function. n_neighbors : int (optional) The number of nearest neighbors to compute for each row (item) of data . A sensible value is chosen by default, depending on the number of items. repulsive_fraction : float (optional) How many repulsive edges to include, relative to the number of attractive edges. 1 means as many repulsive edges as attractive edges. The higher this number, the more uniformly spread out the embedding will be. Defaults to 0.5 for standardized embeddings, and 1 otherwise. (If repulsive_penalty is None , this argument is ignored.) max_distance : float (optional) If not None, neighborhoods are restricted to have a radius no greater than max_distance . init : str or np.ndarray (optional, default 'quadratic') Initialization strategy; np.ndarray, 'quadratic' or 'random'. device : str (optional) Device for the embedding (eg, 'cpu', 'cuda'). verbose : bool If True , print verbose output.","title":"Parameters"},{"location":"topograph/#topo.models.TopOGraph.MDE--returns","text":"torch.tensor A pymde.MDE object, based on the original data. Source code in topo/models.py def MDE ( self , target , data = None , dim = 2 , n_neighbors = None , type = 'isomorphic' , constraint = 'standardized' , init = 'quadratic' , attractive_penalty = penalties . Log1p , repulsive_penalty = penalties . Log , loss = losses . Absolute , repulsive_fraction = None , max_distance = None , device = 'cpu' , verbose = False ): \"\"\" This function constructs an MDE problem for preserving the structure of original data. This MDE problem is well-suited for visualization (using ``dim`` 2 or 3), but can also be used to generate features for machine learning tasks (with ``dim`` = 10, 50, or 100, for example). It yields embeddings in which similar items are near each other, and dissimilar items are not near each other. The original data can either be a data matrix, or a graph. Data matrices should be torch Tensors, NumPy arrays, or scipy sparse matrices; graphs should be instances of ``pymde.Graph``. The MDE problem uses distortion functions derived from weights (i.e., penalties). To obtain an embedding, call the ``embed`` method on the returned ``MDE`` object. To plot it, use ``pymde.plot``. Parameters ---------- data : torch.Tensor, numpy.ndarray, scipy.sparse matrix or pymde.Graph. The original data, a data matrix of shape ``(n_items, n_features)`` or a graph. Neighbors are computed using Euclidean distance if the data is a matrix, or the shortest-path metric if the data is a graph. dim : int. The embedding dimension. Use 2 or 3 for visualization. attractive_penalty : pymde.Function class (or factory). Callable that constructs a distortion function, given positive weights. Typically one of the classes from ``pymde.penalties``, such as ``pymde.penalties.log1p``, ``pymde.penalties.Huber``, or ``pymde.penalties.Quadratic``. repulsive_penalty : pymde.Function class (or factory). Callable that constructs a distortion function, given negative weights. (If ``None``, only positive weights are used.) For example, ``pymde.penalties.Log`` or ``pymde.penalties.InversePower``. constraint : str (optional), default 'standardized'. Constraint to use when optimizing the embedding. Options are 'standardized', 'centered', `None` or a `pymde.constraints.Constraint()` function. n_neighbors : int (optional) The number of nearest neighbors to compute for each row (item) of ``data``. A sensible value is chosen by default, depending on the number of items. repulsive_fraction : float (optional) How many repulsive edges to include, relative to the number of attractive edges. ``1`` means as many repulsive edges as attractive edges. The higher this number, the more uniformly spread out the embedding will be. Defaults to ``0.5`` for standardized embeddings, and ``1`` otherwise. (If ``repulsive_penalty`` is ``None``, this argument is ignored.) max_distance : float (optional) If not None, neighborhoods are restricted to have a radius no greater than ``max_distance``. init : str or np.ndarray (optional, default 'quadratic') Initialization strategy; np.ndarray, 'quadratic' or 'random'. device : str (optional) Device for the embedding (eg, 'cpu', 'cuda'). verbose : bool If ``True``, print verbose output. Returns ------- torch.tensor A ``pymde.MDE`` object, based on the original data. \"\"\" graph = Graph ( target ) if init == 'spectral' : if data is None : print ( 'Spectral initialization requires input data as argument. Falling back to quadratic...' ) init = 'quadratic' else : init = self . spectral_layout ( data , dim ) if constraint == 'standardized' : constraint_use = constraints . Standardized () elif constraint == 'centered' : constraint_use = constraints . Centered () elif isinstance ( constraint , constraints . Constraint ()): constraint_use = constraint else : constraint_use = None if type == 'isomorphic' : emb = mde . IsomorphicMDE ( graph , attractive_penalty = attractive_penalty , repulsive_penalty = repulsive_penalty , embedding_dim = dim , constraint = constraint_use , n_neighbors = n_neighbors , repulsive_fraction = repulsive_fraction , max_distance = max_distance , init = init , device = device , verbose = verbose ) elif type == 'isometric' : if max_distance is None : max_distance = 5e7 emb = mde . IsometricMDE ( graph , embedding_dim = dim , loss = loss , constraint = constraint_use , max_distances = max_distance , device = device , verbose = verbose ) return np . array ( emb )","title":"Returns"},{"location":"topograph/#topo.models.TopOGraph.spectral_layout","text":"Performs a multicomponent spectral layout of the data and the target similarity matrix.","title":"spectral_layout()"},{"location":"topograph/#topo.models.TopOGraph.spectral_layout--parameters","text":"data : input data target : scipy.sparse.csr.csr_matrix. target similarity matrix. dim : int (optional, default 2) number of dimensions to embed into.","title":"Parameters"},{"location":"topograph/#topo.models.TopOGraph.spectral_layout--returns","text":"np.ndarray containing the resulting embedding. Source code in topo/models.py def spectral_layout ( self , data , target , dim = 2 ): \"\"\" Performs a multicomponent spectral layout of the data and the target similarity matrix. Parameters ---------- data : input data target : scipy.sparse.csr.csr_matrix. target similarity matrix. dim : int (optional, default 2) number of dimensions to embed into. Returns ------- np.ndarray containing the resulting embedding. \"\"\" if self . basis == 'diffusion' : spt_layout = spt . spectral_layout ( data , self . DiffBasis . T , dim , self . random_state , metric = \"precomputed\" , ) expansion = 10.0 / np . abs ( spt_layout ) . max () spt_layout = ( spt_layout * expansion ) . astype ( np . float32 ) + self . random_state . normal ( scale = 0.0001 , size = [ self . DiffBasis . T . shape [ 0 ], dim ] ) . astype ( np . float32 ) elif self . basis == 'continuous' : spt_layout = spt . LapEigenmap ( self . ContBasis . K , dim , self . random_state , metric = \"precomputed\" , ) expansion = 10.0 / np . abs ( spt_layout ) . max () spt_layout = ( spt_layout * expansion ) . astype ( np . float32 ) + self . random_state . normal ( scale = 0.0001 , size = [ self . ContBasis . K . shape [ 0 ], dim ] ) . astype ( np . float32 ) return spt_layout","title":"Returns"},{"location":"topograph/#topo.models.TopOGraph.transform","text":"Learns new affinity, topological operators from chosen basis.","title":"transform()"},{"location":"topograph/#topo.models.TopOGraph.transform--parameters","text":"self : TopOGraph instance. base : str, optional. Base to use when building the topological graph. Defaults to the active base ( TopOGraph.basis )","title":"Parameters"},{"location":"topograph/#topo.models.TopOGraph.transform--returns","text":"scipy.sparse.csr.csr_matrix, containing the similarity matrix that encodes the topological graph. Source code in topo/models.py def transform ( self , base ): \"\"\" Learns new affinity, topological operators from chosen basis. Parameters ---------- self : TopOGraph instance. base : str, optional. Base to use when building the topological graph. Defaults to the active base ( `TopOGraph.basis`) Returns ------- scipy.sparse.csr.csr_matrix, containing the similarity matrix that encodes the topological graph. \"\"\" if base is not None : self . basis = base print ( 'Building topological graph...' ) start = time . time () if self . basis == 'continuous' : use_basis = self . CLapMap elif self . basis == 'diffusion' : use_basis = self . MSDiffMap if self . graph == 'diff' : DiffGraph = Diffusor ( n_neighbors = self . graph_knn , alpha = self . alpha , n_jobs = self . n_jobs , ann = self . ann , metric = self . graph_metric , p = self . p , M = self . M , efC = self . efC , efS = self . efS , kernel_use = 'simple' , norm = self . norm , transitions = self . transitions , eigengap = self . eigengap , verbose = self . verbose , plot_spectrum = self . plot_spectrum , cache = False ) . fit ( use_basis ) if self . cache_graph : self . DiffGraph = DiffGraph . T if self . graph == 'cknn' : CknnGraph = cknn_graph ( use_basis , n_neighbors = self . graph_knn , delta = self . delta , metric = self . graph_metric , t = self . t , include_self = True , is_sparse = True ) if self . cache_graph : self . CknnGraph = CknnGraph end = time . time () print ( 'Topological graph extracted in = %f (sec)' % ( end - start )) if self . graph == 'diff' : return DiffGraph . T elif self . graph == 'cknn' : return CknnGraph else : return self","title":"Returns"}]}